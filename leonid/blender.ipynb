{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "# import keras\n",
    "# import tensorflow as tf\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import (Conv1D, Dense, Activation, BatchNormalization, Lambda,\n",
    "#                           Dropout, InputLayer, Input, GlobalMaxPool1D, Flatten)\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = os.path.abspath('../')+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../input/'\n",
    "PRED_TEST_PATH = '../submit/'\n",
    "PRED_TRAIN_PATH = '../submit/'\n",
    "FOLDS_PATH = '../kvr777/folds/'\n",
    "FEATURES_PATH = '../features/'\n",
    "MODEL_NAME = 'leonid09blend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(fpath):\n",
    "    with open(fpath, 'rb') as fin:\n",
    "        return pickle.load(fin)\n",
    "    \n",
    "BASE_PATH = '../submit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artgor_20folds_test_catboost.pkl\n",
      "artgor_20folds_train_catboost.pkl\n",
      "ashevelev_20folds_test_mxnet_RF_cv1789.pkl\n",
      "ashevelev_20folds_test_RF_cv1337_std0028.pkl\n",
      "ashevelev_20folds_train_mxnet_RF_cv1789.pkl\n",
      "ashevelev_20folds_train_RF_cv1337_std0028.pkl\n",
      "blend0820_without_leak_averaged_cv1278_std0007.csv\n",
      "blendings\n",
      "chislov_20folds_test_lgbm_cv1360_std0019.pkl\n",
      "chislov_20folds_train_lgbm_cv1360_std0019.pkl\n",
      "egorlabintcev_20folds_test_lgb_cv13406.pkl\n",
      "fanran_LEAK_TRAINTEST_CV20x5_1.3688.csv\n",
      "insaf_20_fold_train_xgb_gp_clus_cv133_std0021_2ndkernel.pkl\n",
      "insaf_20_folds_test_xgb_gp_clus_cv133_std0021_2ndkernel.pkl\n",
      "iv_cv1404_wo_leak.csv\n",
      "izmaylov_20folds_test_cv1323_std0021.pkl\n",
      "izmaylov_20folds_train_cv1323_std0021.pkl\n",
      "jiazhen-to-armamut-via-gurchetan1000-compiled-leak\n",
      "leonid04_20folds_test_lgbm_cv1317_std0007.pkl\n",
      "leonid04_20folds_train_lgbm_cv1317_std0007.pkl\n",
      "leonid05_20folds_test_averaged_cv1319_std0007.csv\n",
      "leonid05_20folds_test_cv1319_std0007.pkl\n",
      "leonid05_20folds_train_cv1319_std0007.pkl\n",
      "leonid06_20folds_test_averaged_cv1302_std0007.csv\n",
      "leonid06_20folds_test_cv1302_std0007.pkl\n",
      "leonid06_20folds_train_cv1302_std0007.pkl\n",
      "leonid07_20folds_test_averaged_cv1303_std0007.csv\n",
      "leonid07_20folds_test_cv1303_std0007.pkl\n",
      "leonid07_20folds_train_cv1303_std0007.pkl\n",
      "leonid07_fold_errors_cv1303_std0007.csv\n",
      "leonid08_20folds_test_averaged_cv1303_std0007.csv\n",
      "leonid08_20folds_test_cv1303_std0007.pkl\n",
      "leonid08_20folds_train_cv1303_std0007.pkl\n",
      "nefedov_20folds_test_xgb_cv1319_std001.pkl\n",
      "nefedov_20folds_test_xgb_cv1319_std003.pkl\n",
      "nefedov_20folds_test_xgb_cv1328_std003.pkl\n",
      "nefedov_20folds_train_xgb_cv1319_std001.pkl\n",
      "nefedov_20folds_train_xgb_cv1319_std003.pkl\n",
      "nefedov_20folds_train_xgb_cv1328_std003.pkl\n",
      "new_zhav1kwell_ert_cv1318_std003.pkl\n",
      "Nikita\n",
      "nikita_lgb141_cv1348_std0018_oof_and_submit.zip\n",
      "oof_basic_pipeline_20folds_13406.pkl\n",
      "README.md\n",
      "submits_to_check_the_leaks\n",
      "tenich_20folds_test_1dconvnn_cv1561_std0021.pkl\n",
      "tenich_20folds_test_baggedlgb_cv1335_std0021.csv\n",
      "tenich_20folds_train_1dconvnn_cv1561_std0021.pkl\n",
      "tenich_20folds_train_baggedlgb_cv1335_std0021.csv\n",
      "vykhand01_20folds_test_lgbm_cv1322_std0029.pkl\n",
      "vykhand01_20folds_train_lgbm_cv1322_std0029.pkl\n",
      "zhav1kwell_20_folds_extra_trees_cv1318_std003.pkl\n",
      "zhav1kwell_20_folds_train_ert_cv1318_std003.pkl\n",
      "zhav1kwell_rgf_cv13763_std003.csv\n"
     ]
    }
   ],
   "source": [
    "!ls $BASE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_20_cv_splits(data):\n",
    "    #stratify_classes = y\n",
    "    train = pd.read_csv(os.path.join(PATH_TO_DATA, 'input/train.csv'), usecols=['target'])\n",
    "    stratify_classes =  train.target.apply(lambda x: int(np.log10(x)))\n",
    "    splits = {}\n",
    "    for random_state in range(20):\n",
    "        column = np.zeros(data.shape[0])\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=random_state)\n",
    "        for i, (_, test_index) in enumerate(sss.split(data, stratify_classes)):\n",
    "            column[test_index] = i\n",
    "\n",
    "        splits[\"split{}\".format(random_state)] = column\n",
    "\n",
    "    pd.DataFrame(splits, index=data.index).to_csv(os.path.join(PATH_TO_DATA, 'folds/cv_splits_cleandata_stat_bin_red.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # function to generate 100 folds from create_folds_from_cv_splits func\n",
    "def create_folds_from_cv_splits(in_path):\n",
    "    cv_splits = pd.read_csv(os.path.join(PATH_TO_DATA, in_path))\n",
    "    folds_list = []\n",
    "    for ind, i in enumerate(cv_splits.columns[1:]):\n",
    "        folds = list(set(cv_splits[i].values))\n",
    "        folds_list.append([])\n",
    "        for m in folds:\n",
    "            val_idx = list(cv_splits[cv_splits[i]==m].index)\n",
    "            train_idx = list(set(list(cv_splits.index)) - set(val_idx))\n",
    "            folds_list[ind].append((train_idx, val_idx))\n",
    "    with open(os.path.join(PATH_TO_DATA, 'kvr777/folds/custom_cv.pkl'), 'wb') as f:\n",
    "        pickle.dump(folds_list, f)\n",
    "    return folds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CV = True\n",
    "\n",
    "if LOAD_CV:\n",
    "    with open(os.path.join(PATH_TO_DATA, 'kvr777/folds/custom_cv.pkl'), 'rb') as f:\n",
    "        cv_folds = pickle.load(f)\n",
    "else:\n",
    "    get_20_cv_splits(train_df)\n",
    "    cv_folds = create_folds_from_cv_splits(in_path='kvr777/folds/cv_splits_cleandata_stat_bin_red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_features(d):\n",
    "    return np.concatenate([d, \n",
    "                           np.min(d, axis=-1, keepdims=True),\n",
    "                           np.max(d, axis=-1, keepdims=True),\n",
    "                           np.mean(d, axis=-1, keepdims=True),\n",
    "                           np.median(d, axis=-1, keepdims=True)], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ['ashevelev_20folds_test_mxnet_RF_cv1789.pkl',#\n",
    "             'chislov_20folds_test_lgbm_cv1360_std0019.pkl',#\n",
    "             'tenich_20folds_test_1dconvnn_cv1561_std0021.pkl',#\n",
    "             'egorlabintcev_20folds_test_lgb_cv13406.pkl',\n",
    "             'leonid04_20folds_test_lgbm_cv1317_std0007.pkl',\n",
    "             'nefedov_20folds_test_xgb_cv1328_std003.pkl',\n",
    "             'tenich_20folds_test_baggedlgb_cv1335_std0021.csv',\n",
    "             'artgor_20folds_test_catboost.pkl',\n",
    "             'insaf_20_folds_test_xgb_gp_clus_cv133_std0021_2ndkernel.pkl',\n",
    "             'izmaylov_20folds_test_cv1323_std0021.pkl',\n",
    "             'ashevelev_20folds_test_RF_cv1337_std0028.pkl',\n",
    "             'leonid05_20folds_test_cv1319_std0007.pkl',\n",
    "             'leonid06_20folds_test_cv1302_std0007.pkl',\n",
    "             'leonid07_20folds_test_cv1303_std0007.pkl',\n",
    "             'leonid08_20folds_test_cv1303_std0007.pkl',\n",
    "             'Nikita/alexpengxiao_kernel_test_20_folds_lgb_cv1356_std0029.pkl',#\n",
    "             'Nikita/alexpengxiao_kernel_test_20_folds_xgb_cv1348_std0027.pkl',\n",
    "             'Nikita/nikita_b_test_20_folds_xgb_cv1323_std0027.pkl',\n",
    "             'Nikita/nikita_test_20_folds_lgb_cv1340_std0027.pkl',\n",
    "             'Nikita/nikita_test_20_folds_lgb_cv1341_std0026.pkl',\n",
    "             'Nikita/nikita_test_20_folds_lgb_cv1348_std0029.pkl',#\n",
    "             'Nikita/nikita_test_20_folds_lgb_cv1320_std0028.pkl',\n",
    "             'vykhand01_20folds_test_lgbm_cv1322_std0029.pkl',\n",
    "             'nefedov_20folds_train_xgb_cv1319_std003.pkl',\n",
    "             'new_zhav1kwell_ert_cv1318_std003.pkl'\n",
    "             ]\n",
    "\n",
    "train_data = ['ashevelev_20folds_train_mxnet_RF_cv1789.pkl',#\n",
    "              'chislov_20folds_train_lgbm_cv1360_std0019.pkl',#\n",
    "              'tenich_20folds_train_1dconvnn_cv1561_std0021.pkl',#\n",
    "              'oof_basic_pipeline_20folds_13406.pkl',\n",
    "              'leonid04_20folds_train_lgbm_cv1317_std0007.pkl',\n",
    "              'nefedov_20folds_train_xgb_cv1328_std003.pkl',\n",
    "              'tenich_20folds_train_baggedlgb_cv1335_std0021.csv',\n",
    "              'artgor_20folds_train_catboost.pkl',\n",
    "              'insaf_20_fold_train_xgb_gp_clus_cv133_std0021_2ndkernel.pkl',\n",
    "              'izmaylov_20folds_train_cv1323_std0021.pkl',\n",
    "              'ashevelev_20folds_train_RF_cv1337_std0028.pkl',\n",
    "              'leonid05_20folds_train_cv1319_std0007.pkl',\n",
    "              'leonid06_20folds_train_cv1302_std0007.pkl',\n",
    "              'leonid07_20folds_train_cv1303_std0007.pkl',\n",
    "              'leonid08_20folds_train_cv1303_std0007.pkl',\n",
    "              'Nikita/alexpengxiao_kernel_train_20_folds_lgb_cv1356_std0029.pkl',#\n",
    "              'Nikita/alexpengxiao_kernel_train_20_folds_xgb_cv1348_std0027.pkl',\n",
    "              'Nikita/nikita_b_train_20_folds_xgb_cv1323_std0027.pkl',\n",
    "              'Nikita/nikita_train_20_folds_lgb_cv1340_std0027.pkl',\n",
    "              'Nikita/nikita_train_20_folds_lgb_cv1341_std0026.pkl',\n",
    "              'Nikita/nikita_train_20_folds_lgb_cv1348_std0029.pkl',#\n",
    "              'Nikita/nikita_train_20_folds_lgb_cv1320_std0028.pkl',\n",
    "              'vykhand01_20folds_train_lgbm_cv1322_std0029.pkl',\n",
    "              'nefedov_20folds_train_xgb_cv1319_std003.pkl',\n",
    "              'zhav1kwell_20_folds_train_ert_cv1318_std003.pkl'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X done\n",
      "test done\n",
      "(4459, 500) (4459,) (49342, 500)\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_preds = np.array([read_pickle(BASE_PATH + f) for f in train_data])\n",
    "# test_preds = np.array([read_pickle(BASE_PATH + f) for f in test_data])\n",
    "\n",
    "# X = [train_preds[:, i, :] for i in range(20)]\n",
    "# test = [test_preds[:, i, :] for i in range(20)]\n",
    "\n",
    "X = pd.concat([pd.DataFrame(read_pickle(BASE_PATH + f)).T for f in train_data], axis = 1)\n",
    "print('X done')\n",
    "test = pd.concat([pd.DataFrame(read_pickle(BASE_PATH + f)).T for f in test_data], axis = 1)\n",
    "print('test done')\n",
    "# for i in range(20):\n",
    "#     X[i][:, 0] = np.log1p(X[i][:, 0])\n",
    "# #     X[i][:, 4] = np.log1p(X[i][:, 4])\n",
    "#     X[i][:, 7] = np.log1p(X[i][:, 7])\n",
    "#     X[i][:, 10] = np.log1p(X[i][:, 10])\n",
    "    \n",
    "test = np.log1p(test)\n",
    "\n",
    "train_df = pd.read_csv(f'{PATH_TO_DATA}input/train.csv')\n",
    "y = np.log1p(train_df.target)\n",
    "print(X.shape, y.shape, test.shape)\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.832042e+06</td>\n",
       "      <td>2.962803e+06</td>\n",
       "      <td>2.594375e+06</td>\n",
       "      <td>3.143294e+06</td>\n",
       "      <td>2.759915e+06</td>\n",
       "      <td>2.347741e+06</td>\n",
       "      <td>2.486025e+06</td>\n",
       "      <td>2.950824e+06</td>\n",
       "      <td>2.840005e+06</td>\n",
       "      <td>3.105339e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>14.767484</td>\n",
       "      <td>14.747596</td>\n",
       "      <td>14.689561</td>\n",
       "      <td>14.698137</td>\n",
       "      <td>14.669238</td>\n",
       "      <td>14.702120</td>\n",
       "      <td>14.765604</td>\n",
       "      <td>14.676999</td>\n",
       "      <td>14.749081</td>\n",
       "      <td>14.746367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.848837e+06</td>\n",
       "      <td>1.992488e+06</td>\n",
       "      <td>2.207480e+06</td>\n",
       "      <td>2.366350e+06</td>\n",
       "      <td>1.682422e+06</td>\n",
       "      <td>2.009920e+06</td>\n",
       "      <td>1.597052e+06</td>\n",
       "      <td>2.273976e+06</td>\n",
       "      <td>2.317608e+06</td>\n",
       "      <td>1.849735e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>14.668626</td>\n",
       "      <td>14.758232</td>\n",
       "      <td>14.749869</td>\n",
       "      <td>14.690854</td>\n",
       "      <td>14.675423</td>\n",
       "      <td>14.670137</td>\n",
       "      <td>14.556115</td>\n",
       "      <td>14.711833</td>\n",
       "      <td>14.760038</td>\n",
       "      <td>14.784759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.861534e+06</td>\n",
       "      <td>3.325314e+06</td>\n",
       "      <td>3.233440e+06</td>\n",
       "      <td>3.488722e+06</td>\n",
       "      <td>3.502675e+06</td>\n",
       "      <td>3.003053e+06</td>\n",
       "      <td>2.482458e+06</td>\n",
       "      <td>3.461111e+06</td>\n",
       "      <td>3.375058e+06</td>\n",
       "      <td>2.970180e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>14.833868</td>\n",
       "      <td>14.802847</td>\n",
       "      <td>14.837573</td>\n",
       "      <td>14.856471</td>\n",
       "      <td>14.883386</td>\n",
       "      <td>14.881556</td>\n",
       "      <td>14.857297</td>\n",
       "      <td>14.919303</td>\n",
       "      <td>14.832991</td>\n",
       "      <td>14.777038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.697329e+05</td>\n",
       "      <td>7.829859e+05</td>\n",
       "      <td>9.480314e+05</td>\n",
       "      <td>9.194067e+05</td>\n",
       "      <td>9.376369e+05</td>\n",
       "      <td>8.183582e+05</td>\n",
       "      <td>9.959628e+05</td>\n",
       "      <td>9.663052e+05</td>\n",
       "      <td>9.104189e+05</td>\n",
       "      <td>8.682808e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>13.733369</td>\n",
       "      <td>13.784835</td>\n",
       "      <td>13.582351</td>\n",
       "      <td>13.785400</td>\n",
       "      <td>13.606475</td>\n",
       "      <td>13.600490</td>\n",
       "      <td>13.814911</td>\n",
       "      <td>13.738850</td>\n",
       "      <td>13.692593</td>\n",
       "      <td>13.706627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.146797e+06</td>\n",
       "      <td>3.992424e+06</td>\n",
       "      <td>3.754497e+06</td>\n",
       "      <td>3.889899e+06</td>\n",
       "      <td>4.201063e+06</td>\n",
       "      <td>4.009800e+06</td>\n",
       "      <td>3.958549e+06</td>\n",
       "      <td>3.205308e+06</td>\n",
       "      <td>3.628521e+06</td>\n",
       "      <td>3.838257e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>14.976959</td>\n",
       "      <td>14.903025</td>\n",
       "      <td>15.062302</td>\n",
       "      <td>14.962032</td>\n",
       "      <td>14.955784</td>\n",
       "      <td>15.001003</td>\n",
       "      <td>14.987674</td>\n",
       "      <td>14.977360</td>\n",
       "      <td>14.982414</td>\n",
       "      <td>15.041061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.232196e+06</td>\n",
       "      <td>2.361613e+06</td>\n",
       "      <td>1.291814e+06</td>\n",
       "      <td>1.445467e+06</td>\n",
       "      <td>2.341916e+06</td>\n",
       "      <td>2.540010e+06</td>\n",
       "      <td>3.402041e+06</td>\n",
       "      <td>3.463169e+06</td>\n",
       "      <td>3.390596e+06</td>\n",
       "      <td>2.923406e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>14.720076</td>\n",
       "      <td>14.736186</td>\n",
       "      <td>14.593845</td>\n",
       "      <td>14.689543</td>\n",
       "      <td>14.743393</td>\n",
       "      <td>14.720136</td>\n",
       "      <td>14.833548</td>\n",
       "      <td>14.811833</td>\n",
       "      <td>14.853132</td>\n",
       "      <td>14.768808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.277827e+05</td>\n",
       "      <td>1.483515e+05</td>\n",
       "      <td>2.438786e+05</td>\n",
       "      <td>2.269260e+05</td>\n",
       "      <td>2.841412e+05</td>\n",
       "      <td>2.689844e+05</td>\n",
       "      <td>2.535328e+05</td>\n",
       "      <td>2.734043e+05</td>\n",
       "      <td>3.513708e+05</td>\n",
       "      <td>2.852178e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>11.636044</td>\n",
       "      <td>11.912775</td>\n",
       "      <td>11.729613</td>\n",
       "      <td>11.755063</td>\n",
       "      <td>11.767263</td>\n",
       "      <td>11.620009</td>\n",
       "      <td>11.693450</td>\n",
       "      <td>11.719832</td>\n",
       "      <td>11.714067</td>\n",
       "      <td>11.869060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.540922e+06</td>\n",
       "      <td>4.509616e+06</td>\n",
       "      <td>6.118600e+06</td>\n",
       "      <td>4.233030e+06</td>\n",
       "      <td>4.416945e+06</td>\n",
       "      <td>5.575118e+06</td>\n",
       "      <td>3.323755e+06</td>\n",
       "      <td>4.211592e+06</td>\n",
       "      <td>4.541200e+06</td>\n",
       "      <td>4.511588e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>15.058416</td>\n",
       "      <td>15.168764</td>\n",
       "      <td>15.247369</td>\n",
       "      <td>15.067891</td>\n",
       "      <td>15.094034</td>\n",
       "      <td>15.254368</td>\n",
       "      <td>15.044469</td>\n",
       "      <td>15.222354</td>\n",
       "      <td>15.117875</td>\n",
       "      <td>15.151168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.221663e+06</td>\n",
       "      <td>2.021820e+06</td>\n",
       "      <td>3.202764e+06</td>\n",
       "      <td>2.064614e+06</td>\n",
       "      <td>2.211733e+06</td>\n",
       "      <td>2.806549e+06</td>\n",
       "      <td>2.084794e+06</td>\n",
       "      <td>4.172736e+06</td>\n",
       "      <td>2.977960e+06</td>\n",
       "      <td>2.723344e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>14.468447</td>\n",
       "      <td>14.429849</td>\n",
       "      <td>14.495700</td>\n",
       "      <td>14.437555</td>\n",
       "      <td>14.435801</td>\n",
       "      <td>14.461508</td>\n",
       "      <td>14.442009</td>\n",
       "      <td>14.579465</td>\n",
       "      <td>14.528498</td>\n",
       "      <td>14.509268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.597815e+05</td>\n",
       "      <td>9.002974e+05</td>\n",
       "      <td>1.123993e+06</td>\n",
       "      <td>9.826926e+05</td>\n",
       "      <td>7.962081e+05</td>\n",
       "      <td>1.003426e+06</td>\n",
       "      <td>1.071410e+06</td>\n",
       "      <td>9.151741e+05</td>\n",
       "      <td>1.090330e+06</td>\n",
       "      <td>1.012308e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>14.530877</td>\n",
       "      <td>14.648685</td>\n",
       "      <td>14.477749</td>\n",
       "      <td>14.610083</td>\n",
       "      <td>14.509849</td>\n",
       "      <td>14.541207</td>\n",
       "      <td>14.678700</td>\n",
       "      <td>14.609271</td>\n",
       "      <td>14.589014</td>\n",
       "      <td>14.614146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1             2             3             4   \\\n",
       "0  2.832042e+06  2.962803e+06  2.594375e+06  3.143294e+06  2.759915e+06   \n",
       "1  1.848837e+06  1.992488e+06  2.207480e+06  2.366350e+06  1.682422e+06   \n",
       "2  2.861534e+06  3.325314e+06  3.233440e+06  3.488722e+06  3.502675e+06   \n",
       "3  8.697329e+05  7.829859e+05  9.480314e+05  9.194067e+05  9.376369e+05   \n",
       "4  4.146797e+06  3.992424e+06  3.754497e+06  3.889899e+06  4.201063e+06   \n",
       "5  3.232196e+06  2.361613e+06  1.291814e+06  1.445467e+06  2.341916e+06   \n",
       "6  3.277827e+05  1.483515e+05  2.438786e+05  2.269260e+05  2.841412e+05   \n",
       "7  4.540922e+06  4.509616e+06  6.118600e+06  4.233030e+06  4.416945e+06   \n",
       "8  2.221663e+06  2.021820e+06  3.202764e+06  2.064614e+06  2.211733e+06   \n",
       "9  9.597815e+05  9.002974e+05  1.123993e+06  9.826926e+05  7.962081e+05   \n",
       "\n",
       "             5             6             7             8             9   \\\n",
       "0  2.347741e+06  2.486025e+06  2.950824e+06  2.840005e+06  3.105339e+06   \n",
       "1  2.009920e+06  1.597052e+06  2.273976e+06  2.317608e+06  1.849735e+06   \n",
       "2  3.003053e+06  2.482458e+06  3.461111e+06  3.375058e+06  2.970180e+06   \n",
       "3  8.183582e+05  9.959628e+05  9.663052e+05  9.104189e+05  8.682808e+05   \n",
       "4  4.009800e+06  3.958549e+06  3.205308e+06  3.628521e+06  3.838257e+06   \n",
       "5  2.540010e+06  3.402041e+06  3.463169e+06  3.390596e+06  2.923406e+06   \n",
       "6  2.689844e+05  2.535328e+05  2.734043e+05  3.513708e+05  2.852178e+05   \n",
       "7  5.575118e+06  3.323755e+06  4.211592e+06  4.541200e+06  4.511588e+06   \n",
       "8  2.806549e+06  2.084794e+06  4.172736e+06  2.977960e+06  2.723344e+06   \n",
       "9  1.003426e+06  1.071410e+06  9.151741e+05  1.090330e+06  1.012308e+06   \n",
       "\n",
       "     ...             10         11         12         13         14  \\\n",
       "0    ...      14.767484  14.747596  14.689561  14.698137  14.669238   \n",
       "1    ...      14.668626  14.758232  14.749869  14.690854  14.675423   \n",
       "2    ...      14.833868  14.802847  14.837573  14.856471  14.883386   \n",
       "3    ...      13.733369  13.784835  13.582351  13.785400  13.606475   \n",
       "4    ...      14.976959  14.903025  15.062302  14.962032  14.955784   \n",
       "5    ...      14.720076  14.736186  14.593845  14.689543  14.743393   \n",
       "6    ...      11.636044  11.912775  11.729613  11.755063  11.767263   \n",
       "7    ...      15.058416  15.168764  15.247369  15.067891  15.094034   \n",
       "8    ...      14.468447  14.429849  14.495700  14.437555  14.435801   \n",
       "9    ...      14.530877  14.648685  14.477749  14.610083  14.509849   \n",
       "\n",
       "          15         16         17         18         19  \n",
       "0  14.702120  14.765604  14.676999  14.749081  14.746367  \n",
       "1  14.670137  14.556115  14.711833  14.760038  14.784759  \n",
       "2  14.881556  14.857297  14.919303  14.832991  14.777038  \n",
       "3  13.600490  13.814911  13.738850  13.692593  13.706627  \n",
       "4  15.001003  14.987674  14.977360  14.982414  15.041061  \n",
       "5  14.720136  14.833548  14.811833  14.853132  14.768808  \n",
       "6  11.620009  11.693450  11.719832  11.714067  11.869060  \n",
       "7  15.254368  15.044469  15.222354  15.117875  15.151168  \n",
       "8  14.461508  14.442009  14.579465  14.528498  14.509268  \n",
       "9  14.541207  14.678700  14.609271  14.589014  14.614146  \n",
       "\n",
       "[10 rows x 500 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.911401</td>\n",
       "      <td>14.911401</td>\n",
       "      <td>14.911401</td>\n",
       "      <td>14.911401</td>\n",
       "      <td>14.911401</td>\n",
       "      <td>14.911401</td>\n",
       "      <td>14.911401</td>\n",
       "      <td>14.911401</td>\n",
       "      <td>14.911401</td>\n",
       "      <td>14.911401</td>\n",
       "      <td>...</td>\n",
       "      <td>14.701627</td>\n",
       "      <td>14.684726</td>\n",
       "      <td>14.725431</td>\n",
       "      <td>14.745341</td>\n",
       "      <td>14.705209</td>\n",
       "      <td>14.716565</td>\n",
       "      <td>14.703124</td>\n",
       "      <td>14.728486</td>\n",
       "      <td>14.734382</td>\n",
       "      <td>14.687252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.277120</td>\n",
       "      <td>14.277120</td>\n",
       "      <td>14.277120</td>\n",
       "      <td>14.277120</td>\n",
       "      <td>14.277120</td>\n",
       "      <td>14.277120</td>\n",
       "      <td>14.277120</td>\n",
       "      <td>14.277120</td>\n",
       "      <td>14.277120</td>\n",
       "      <td>14.277120</td>\n",
       "      <td>...</td>\n",
       "      <td>13.935890</td>\n",
       "      <td>13.927885</td>\n",
       "      <td>13.943929</td>\n",
       "      <td>13.952025</td>\n",
       "      <td>13.946252</td>\n",
       "      <td>13.961022</td>\n",
       "      <td>13.921171</td>\n",
       "      <td>13.959312</td>\n",
       "      <td>13.967677</td>\n",
       "      <td>13.943854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.720255</td>\n",
       "      <td>14.720255</td>\n",
       "      <td>14.720255</td>\n",
       "      <td>14.720255</td>\n",
       "      <td>14.720255</td>\n",
       "      <td>14.720255</td>\n",
       "      <td>14.720255</td>\n",
       "      <td>14.720255</td>\n",
       "      <td>14.720255</td>\n",
       "      <td>14.720255</td>\n",
       "      <td>...</td>\n",
       "      <td>14.506620</td>\n",
       "      <td>14.482190</td>\n",
       "      <td>14.504688</td>\n",
       "      <td>14.518320</td>\n",
       "      <td>14.504975</td>\n",
       "      <td>14.484002</td>\n",
       "      <td>14.496314</td>\n",
       "      <td>14.537933</td>\n",
       "      <td>14.519676</td>\n",
       "      <td>14.526373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.241589</td>\n",
       "      <td>15.241589</td>\n",
       "      <td>15.241589</td>\n",
       "      <td>15.241589</td>\n",
       "      <td>15.241589</td>\n",
       "      <td>15.241589</td>\n",
       "      <td>15.241589</td>\n",
       "      <td>15.241589</td>\n",
       "      <td>15.241589</td>\n",
       "      <td>15.241589</td>\n",
       "      <td>...</td>\n",
       "      <td>15.306287</td>\n",
       "      <td>15.301992</td>\n",
       "      <td>15.288202</td>\n",
       "      <td>15.273772</td>\n",
       "      <td>15.290146</td>\n",
       "      <td>15.314761</td>\n",
       "      <td>15.277124</td>\n",
       "      <td>15.313032</td>\n",
       "      <td>15.298357</td>\n",
       "      <td>15.314534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.684171</td>\n",
       "      <td>14.684171</td>\n",
       "      <td>14.684171</td>\n",
       "      <td>14.684171</td>\n",
       "      <td>14.684171</td>\n",
       "      <td>14.684171</td>\n",
       "      <td>14.684171</td>\n",
       "      <td>14.684171</td>\n",
       "      <td>14.684171</td>\n",
       "      <td>14.684171</td>\n",
       "      <td>...</td>\n",
       "      <td>14.358911</td>\n",
       "      <td>14.347630</td>\n",
       "      <td>14.396201</td>\n",
       "      <td>14.390430</td>\n",
       "      <td>14.378745</td>\n",
       "      <td>14.365091</td>\n",
       "      <td>14.343846</td>\n",
       "      <td>14.363702</td>\n",
       "      <td>14.373572</td>\n",
       "      <td>14.365174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.921523</td>\n",
       "      <td>14.921523</td>\n",
       "      <td>14.921523</td>\n",
       "      <td>14.921523</td>\n",
       "      <td>14.921523</td>\n",
       "      <td>14.921523</td>\n",
       "      <td>14.921523</td>\n",
       "      <td>14.921523</td>\n",
       "      <td>14.921523</td>\n",
       "      <td>14.921523</td>\n",
       "      <td>...</td>\n",
       "      <td>14.151391</td>\n",
       "      <td>14.116652</td>\n",
       "      <td>14.136599</td>\n",
       "      <td>14.154159</td>\n",
       "      <td>14.149712</td>\n",
       "      <td>14.137335</td>\n",
       "      <td>14.119978</td>\n",
       "      <td>14.144437</td>\n",
       "      <td>14.137026</td>\n",
       "      <td>14.145014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.711668</td>\n",
       "      <td>14.711668</td>\n",
       "      <td>14.711668</td>\n",
       "      <td>14.711668</td>\n",
       "      <td>14.711668</td>\n",
       "      <td>14.711668</td>\n",
       "      <td>14.711668</td>\n",
       "      <td>14.711668</td>\n",
       "      <td>14.711668</td>\n",
       "      <td>14.711668</td>\n",
       "      <td>...</td>\n",
       "      <td>14.129798</td>\n",
       "      <td>14.109647</td>\n",
       "      <td>14.147490</td>\n",
       "      <td>14.130360</td>\n",
       "      <td>14.132148</td>\n",
       "      <td>14.147171</td>\n",
       "      <td>14.154362</td>\n",
       "      <td>14.137867</td>\n",
       "      <td>14.166525</td>\n",
       "      <td>14.141281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.022814</td>\n",
       "      <td>15.022814</td>\n",
       "      <td>15.022814</td>\n",
       "      <td>15.022814</td>\n",
       "      <td>15.022814</td>\n",
       "      <td>15.022814</td>\n",
       "      <td>15.022814</td>\n",
       "      <td>15.022814</td>\n",
       "      <td>15.022814</td>\n",
       "      <td>15.022814</td>\n",
       "      <td>...</td>\n",
       "      <td>14.639775</td>\n",
       "      <td>14.626651</td>\n",
       "      <td>14.640331</td>\n",
       "      <td>14.650334</td>\n",
       "      <td>14.626303</td>\n",
       "      <td>14.612437</td>\n",
       "      <td>14.631202</td>\n",
       "      <td>14.642324</td>\n",
       "      <td>14.634284</td>\n",
       "      <td>14.623593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.874045</td>\n",
       "      <td>14.874045</td>\n",
       "      <td>14.874045</td>\n",
       "      <td>14.874045</td>\n",
       "      <td>14.874045</td>\n",
       "      <td>14.874045</td>\n",
       "      <td>14.874045</td>\n",
       "      <td>14.874045</td>\n",
       "      <td>14.874045</td>\n",
       "      <td>14.874045</td>\n",
       "      <td>...</td>\n",
       "      <td>14.455282</td>\n",
       "      <td>14.422562</td>\n",
       "      <td>14.451239</td>\n",
       "      <td>14.460968</td>\n",
       "      <td>14.444224</td>\n",
       "      <td>14.420442</td>\n",
       "      <td>14.427907</td>\n",
       "      <td>14.434768</td>\n",
       "      <td>14.436729</td>\n",
       "      <td>14.440587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.803345</td>\n",
       "      <td>14.803345</td>\n",
       "      <td>14.803345</td>\n",
       "      <td>14.803345</td>\n",
       "      <td>14.803345</td>\n",
       "      <td>14.803345</td>\n",
       "      <td>14.803345</td>\n",
       "      <td>14.803345</td>\n",
       "      <td>14.803345</td>\n",
       "      <td>14.803345</td>\n",
       "      <td>...</td>\n",
       "      <td>14.770785</td>\n",
       "      <td>14.816562</td>\n",
       "      <td>14.789981</td>\n",
       "      <td>14.793529</td>\n",
       "      <td>14.813369</td>\n",
       "      <td>14.785049</td>\n",
       "      <td>14.789854</td>\n",
       "      <td>14.779319</td>\n",
       "      <td>14.812291</td>\n",
       "      <td>14.798659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14.900939</td>\n",
       "      <td>14.900939</td>\n",
       "      <td>14.900939</td>\n",
       "      <td>14.900939</td>\n",
       "      <td>14.900939</td>\n",
       "      <td>14.900939</td>\n",
       "      <td>14.900939</td>\n",
       "      <td>14.900939</td>\n",
       "      <td>14.900939</td>\n",
       "      <td>14.900939</td>\n",
       "      <td>...</td>\n",
       "      <td>14.635835</td>\n",
       "      <td>14.657946</td>\n",
       "      <td>14.669085</td>\n",
       "      <td>14.651330</td>\n",
       "      <td>14.659712</td>\n",
       "      <td>14.678708</td>\n",
       "      <td>14.675780</td>\n",
       "      <td>14.669277</td>\n",
       "      <td>14.686767</td>\n",
       "      <td>14.626737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.051897</td>\n",
       "      <td>16.051897</td>\n",
       "      <td>16.051897</td>\n",
       "      <td>16.051897</td>\n",
       "      <td>16.051897</td>\n",
       "      <td>16.051897</td>\n",
       "      <td>16.051897</td>\n",
       "      <td>16.051897</td>\n",
       "      <td>16.051897</td>\n",
       "      <td>16.051897</td>\n",
       "      <td>...</td>\n",
       "      <td>15.663194</td>\n",
       "      <td>15.598440</td>\n",
       "      <td>15.609218</td>\n",
       "      <td>15.638171</td>\n",
       "      <td>15.626457</td>\n",
       "      <td>15.635748</td>\n",
       "      <td>15.588794</td>\n",
       "      <td>15.634042</td>\n",
       "      <td>15.641942</td>\n",
       "      <td>15.628812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.862155</td>\n",
       "      <td>14.862155</td>\n",
       "      <td>14.862155</td>\n",
       "      <td>14.862155</td>\n",
       "      <td>14.862155</td>\n",
       "      <td>14.862155</td>\n",
       "      <td>14.862155</td>\n",
       "      <td>14.862155</td>\n",
       "      <td>14.862155</td>\n",
       "      <td>14.862155</td>\n",
       "      <td>...</td>\n",
       "      <td>14.110367</td>\n",
       "      <td>14.094973</td>\n",
       "      <td>14.103433</td>\n",
       "      <td>14.103812</td>\n",
       "      <td>14.090095</td>\n",
       "      <td>14.102676</td>\n",
       "      <td>14.115409</td>\n",
       "      <td>14.095655</td>\n",
       "      <td>14.093895</td>\n",
       "      <td>14.096475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.679606</td>\n",
       "      <td>14.679606</td>\n",
       "      <td>14.679606</td>\n",
       "      <td>14.679606</td>\n",
       "      <td>14.679606</td>\n",
       "      <td>14.679606</td>\n",
       "      <td>14.679606</td>\n",
       "      <td>14.679606</td>\n",
       "      <td>14.679606</td>\n",
       "      <td>14.679606</td>\n",
       "      <td>...</td>\n",
       "      <td>14.125605</td>\n",
       "      <td>14.144699</td>\n",
       "      <td>14.160533</td>\n",
       "      <td>14.182462</td>\n",
       "      <td>14.193737</td>\n",
       "      <td>14.185776</td>\n",
       "      <td>14.159520</td>\n",
       "      <td>14.161935</td>\n",
       "      <td>14.170743</td>\n",
       "      <td>14.174470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.734568</td>\n",
       "      <td>14.734568</td>\n",
       "      <td>14.734568</td>\n",
       "      <td>14.734568</td>\n",
       "      <td>14.734568</td>\n",
       "      <td>14.734568</td>\n",
       "      <td>14.734568</td>\n",
       "      <td>14.734568</td>\n",
       "      <td>14.734568</td>\n",
       "      <td>14.734568</td>\n",
       "      <td>...</td>\n",
       "      <td>14.280818</td>\n",
       "      <td>14.306056</td>\n",
       "      <td>14.247136</td>\n",
       "      <td>14.263209</td>\n",
       "      <td>14.236431</td>\n",
       "      <td>14.262903</td>\n",
       "      <td>14.249505</td>\n",
       "      <td>14.279359</td>\n",
       "      <td>14.283890</td>\n",
       "      <td>14.282768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.265655</td>\n",
       "      <td>14.265655</td>\n",
       "      <td>14.265655</td>\n",
       "      <td>14.265655</td>\n",
       "      <td>14.265655</td>\n",
       "      <td>14.265655</td>\n",
       "      <td>14.265655</td>\n",
       "      <td>14.265655</td>\n",
       "      <td>14.265655</td>\n",
       "      <td>14.265655</td>\n",
       "      <td>...</td>\n",
       "      <td>14.629117</td>\n",
       "      <td>14.643552</td>\n",
       "      <td>14.635595</td>\n",
       "      <td>14.634385</td>\n",
       "      <td>14.621485</td>\n",
       "      <td>14.634476</td>\n",
       "      <td>14.595540</td>\n",
       "      <td>14.623792</td>\n",
       "      <td>14.619869</td>\n",
       "      <td>14.639064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.800797</td>\n",
       "      <td>14.800797</td>\n",
       "      <td>14.800797</td>\n",
       "      <td>14.800797</td>\n",
       "      <td>14.800797</td>\n",
       "      <td>14.800797</td>\n",
       "      <td>14.800797</td>\n",
       "      <td>14.800797</td>\n",
       "      <td>14.800797</td>\n",
       "      <td>14.800797</td>\n",
       "      <td>...</td>\n",
       "      <td>14.874824</td>\n",
       "      <td>14.903828</td>\n",
       "      <td>14.926103</td>\n",
       "      <td>14.895816</td>\n",
       "      <td>14.900879</td>\n",
       "      <td>14.893586</td>\n",
       "      <td>14.917560</td>\n",
       "      <td>14.905796</td>\n",
       "      <td>14.904449</td>\n",
       "      <td>14.886355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.118783</td>\n",
       "      <td>14.118783</td>\n",
       "      <td>14.118783</td>\n",
       "      <td>14.118783</td>\n",
       "      <td>14.118783</td>\n",
       "      <td>14.118783</td>\n",
       "      <td>14.118783</td>\n",
       "      <td>14.118783</td>\n",
       "      <td>14.118783</td>\n",
       "      <td>14.118783</td>\n",
       "      <td>...</td>\n",
       "      <td>13.715745</td>\n",
       "      <td>13.764402</td>\n",
       "      <td>13.742323</td>\n",
       "      <td>13.728988</td>\n",
       "      <td>13.721463</td>\n",
       "      <td>13.707240</td>\n",
       "      <td>13.705676</td>\n",
       "      <td>13.687635</td>\n",
       "      <td>13.718789</td>\n",
       "      <td>13.699455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.928954</td>\n",
       "      <td>14.928954</td>\n",
       "      <td>14.928954</td>\n",
       "      <td>14.928954</td>\n",
       "      <td>14.928954</td>\n",
       "      <td>14.928954</td>\n",
       "      <td>14.928954</td>\n",
       "      <td>14.928954</td>\n",
       "      <td>14.928954</td>\n",
       "      <td>14.928954</td>\n",
       "      <td>...</td>\n",
       "      <td>14.007523</td>\n",
       "      <td>13.996146</td>\n",
       "      <td>13.966715</td>\n",
       "      <td>13.983037</td>\n",
       "      <td>13.950491</td>\n",
       "      <td>13.983510</td>\n",
       "      <td>14.016588</td>\n",
       "      <td>13.987072</td>\n",
       "      <td>14.010705</td>\n",
       "      <td>13.965826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14.795818</td>\n",
       "      <td>14.795818</td>\n",
       "      <td>14.795818</td>\n",
       "      <td>14.795818</td>\n",
       "      <td>14.795818</td>\n",
       "      <td>14.795818</td>\n",
       "      <td>14.795818</td>\n",
       "      <td>14.795818</td>\n",
       "      <td>14.795818</td>\n",
       "      <td>14.795818</td>\n",
       "      <td>...</td>\n",
       "      <td>14.674688</td>\n",
       "      <td>14.639817</td>\n",
       "      <td>14.663099</td>\n",
       "      <td>14.645596</td>\n",
       "      <td>14.634473</td>\n",
       "      <td>14.649672</td>\n",
       "      <td>14.635825</td>\n",
       "      <td>14.647256</td>\n",
       "      <td>14.654297</td>\n",
       "      <td>14.657233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5   \\\n",
       "0   14.911401  14.911401  14.911401  14.911401  14.911401  14.911401   \n",
       "1   14.277120  14.277120  14.277120  14.277120  14.277120  14.277120   \n",
       "2   14.720255  14.720255  14.720255  14.720255  14.720255  14.720255   \n",
       "3   15.241589  15.241589  15.241589  15.241589  15.241589  15.241589   \n",
       "4   14.684171  14.684171  14.684171  14.684171  14.684171  14.684171   \n",
       "5   14.921523  14.921523  14.921523  14.921523  14.921523  14.921523   \n",
       "6   14.711668  14.711668  14.711668  14.711668  14.711668  14.711668   \n",
       "7   15.022814  15.022814  15.022814  15.022814  15.022814  15.022814   \n",
       "8   14.874045  14.874045  14.874045  14.874045  14.874045  14.874045   \n",
       "9   14.803345  14.803345  14.803345  14.803345  14.803345  14.803345   \n",
       "10  14.900939  14.900939  14.900939  14.900939  14.900939  14.900939   \n",
       "11  16.051897  16.051897  16.051897  16.051897  16.051897  16.051897   \n",
       "12  14.862155  14.862155  14.862155  14.862155  14.862155  14.862155   \n",
       "13  14.679606  14.679606  14.679606  14.679606  14.679606  14.679606   \n",
       "14  14.734568  14.734568  14.734568  14.734568  14.734568  14.734568   \n",
       "15  14.265655  14.265655  14.265655  14.265655  14.265655  14.265655   \n",
       "16  14.800797  14.800797  14.800797  14.800797  14.800797  14.800797   \n",
       "17  14.118783  14.118783  14.118783  14.118783  14.118783  14.118783   \n",
       "18  14.928954  14.928954  14.928954  14.928954  14.928954  14.928954   \n",
       "19  14.795818  14.795818  14.795818  14.795818  14.795818  14.795818   \n",
       "\n",
       "           6          7          8          9     ...             10  \\\n",
       "0   14.911401  14.911401  14.911401  14.911401    ...      14.701627   \n",
       "1   14.277120  14.277120  14.277120  14.277120    ...      13.935890   \n",
       "2   14.720255  14.720255  14.720255  14.720255    ...      14.506620   \n",
       "3   15.241589  15.241589  15.241589  15.241589    ...      15.306287   \n",
       "4   14.684171  14.684171  14.684171  14.684171    ...      14.358911   \n",
       "5   14.921523  14.921523  14.921523  14.921523    ...      14.151391   \n",
       "6   14.711668  14.711668  14.711668  14.711668    ...      14.129798   \n",
       "7   15.022814  15.022814  15.022814  15.022814    ...      14.639775   \n",
       "8   14.874045  14.874045  14.874045  14.874045    ...      14.455282   \n",
       "9   14.803345  14.803345  14.803345  14.803345    ...      14.770785   \n",
       "10  14.900939  14.900939  14.900939  14.900939    ...      14.635835   \n",
       "11  16.051897  16.051897  16.051897  16.051897    ...      15.663194   \n",
       "12  14.862155  14.862155  14.862155  14.862155    ...      14.110367   \n",
       "13  14.679606  14.679606  14.679606  14.679606    ...      14.125605   \n",
       "14  14.734568  14.734568  14.734568  14.734568    ...      14.280818   \n",
       "15  14.265655  14.265655  14.265655  14.265655    ...      14.629117   \n",
       "16  14.800797  14.800797  14.800797  14.800797    ...      14.874824   \n",
       "17  14.118783  14.118783  14.118783  14.118783    ...      13.715745   \n",
       "18  14.928954  14.928954  14.928954  14.928954    ...      14.007523   \n",
       "19  14.795818  14.795818  14.795818  14.795818    ...      14.674688   \n",
       "\n",
       "           11         12         13         14         15         16  \\\n",
       "0   14.684726  14.725431  14.745341  14.705209  14.716565  14.703124   \n",
       "1   13.927885  13.943929  13.952025  13.946252  13.961022  13.921171   \n",
       "2   14.482190  14.504688  14.518320  14.504975  14.484002  14.496314   \n",
       "3   15.301992  15.288202  15.273772  15.290146  15.314761  15.277124   \n",
       "4   14.347630  14.396201  14.390430  14.378745  14.365091  14.343846   \n",
       "5   14.116652  14.136599  14.154159  14.149712  14.137335  14.119978   \n",
       "6   14.109647  14.147490  14.130360  14.132148  14.147171  14.154362   \n",
       "7   14.626651  14.640331  14.650334  14.626303  14.612437  14.631202   \n",
       "8   14.422562  14.451239  14.460968  14.444224  14.420442  14.427907   \n",
       "9   14.816562  14.789981  14.793529  14.813369  14.785049  14.789854   \n",
       "10  14.657946  14.669085  14.651330  14.659712  14.678708  14.675780   \n",
       "11  15.598440  15.609218  15.638171  15.626457  15.635748  15.588794   \n",
       "12  14.094973  14.103433  14.103812  14.090095  14.102676  14.115409   \n",
       "13  14.144699  14.160533  14.182462  14.193737  14.185776  14.159520   \n",
       "14  14.306056  14.247136  14.263209  14.236431  14.262903  14.249505   \n",
       "15  14.643552  14.635595  14.634385  14.621485  14.634476  14.595540   \n",
       "16  14.903828  14.926103  14.895816  14.900879  14.893586  14.917560   \n",
       "17  13.764402  13.742323  13.728988  13.721463  13.707240  13.705676   \n",
       "18  13.996146  13.966715  13.983037  13.950491  13.983510  14.016588   \n",
       "19  14.639817  14.663099  14.645596  14.634473  14.649672  14.635825   \n",
       "\n",
       "           17         18         19  \n",
       "0   14.728486  14.734382  14.687252  \n",
       "1   13.959312  13.967677  13.943854  \n",
       "2   14.537933  14.519676  14.526373  \n",
       "3   15.313032  15.298357  15.314534  \n",
       "4   14.363702  14.373572  14.365174  \n",
       "5   14.144437  14.137026  14.145014  \n",
       "6   14.137867  14.166525  14.141281  \n",
       "7   14.642324  14.634284  14.623593  \n",
       "8   14.434768  14.436729  14.440587  \n",
       "9   14.779319  14.812291  14.798659  \n",
       "10  14.669277  14.686767  14.626737  \n",
       "11  15.634042  15.641942  15.628812  \n",
       "12  14.095655  14.093895  14.096475  \n",
       "13  14.161935  14.170743  14.174470  \n",
       "14  14.279359  14.283890  14.282768  \n",
       "15  14.623792  14.619869  14.639064  \n",
       "16  14.905796  14.904449  14.886355  \n",
       "17  13.687635  13.718789  13.699455  \n",
       "18  13.987072  14.010705  13.965826  \n",
       "19  14.647256  14.654297  14.657233  \n",
       "\n",
       "[20 rows x 500 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4459 entries, 0 to 4458\n",
      "Columns: 500 entries, 0 to 19\n",
      "dtypes: float64(500)\n",
      "memory usage: 17.0 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49342 entries, 0 to 49341\n",
      "Columns: 500 entries, 0 to 19\n",
      "dtypes: float64(500)\n",
      "memory usage: 188.2 MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train_preds)):\n",
    "#     cur_scores = [mean_squared_error(y, X[j][:, i]) ** 0.5 for j in range(20)]\n",
    "#     print(np.mean(cur_scores), np.std(cur_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.min(), test.max(), np.array(X).min(), np.array(X).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model():\n",
    "# #     input = Input(shape=(len(test_preds), ))\n",
    "    \n",
    "# #     dense_1 = Dense(4, activation='relu')(input)\n",
    "    \n",
    "# #     output = Dense(1)(dense_1)\n",
    "    \n",
    "# #     model = Model(input, output)\n",
    "    \n",
    "# #     model.compile('adam', 'mse')\n",
    "    \n",
    "#     return Ridge(alpha=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_model(train_X, train_y, val_X, val_y, test_X):\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     model = get_model()\n",
    "#     model.fit(train_X, train_y)#, batch_size=64, epochs=100, validation_data=(val_X, val_y))\n",
    "    \n",
    "#     print('Model training done in {} seconds.'.format(time.time() - start_time))\n",
    "    \n",
    "#     pred_test_y = model.predict(test_X).ravel()\n",
    "#     pred_oof_log = model.predict(val_X).ravel()\n",
    "    \n",
    "#     print(train_y.max(), val_y.max())\n",
    "\n",
    "#     return pred_test_y, pred_oof_log, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_X, train_y, val_X, val_y, test_X, seed = RANDOM_STATE):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 512,#40\n",
    "        'max_depth': 5,\n",
    "        \"learning_rate\" : 0.004,#0.005\n",
    "        \"bagging_fraction\" : 0.45,\n",
    "        \"feature_fraction\" : 0.97,#0.6\n",
    "        \"bagging_freq\" : 1,\n",
    "        \"verbosity\" : -1,\n",
    "        'num_threads' : 4,\n",
    "        \"seed\": seed\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, 10000, \n",
    "                      valid_sets=[lgtrain, lgval], \n",
    "                      verbose_eval=200, \n",
    "                      early_stopping_rounds=200)\n",
    "    print('Model training done in {} seconds.'.format(time.time() - start_time))\n",
    "    \n",
    "    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n",
    "    pred_oof_log = model.predict(val_X, num_iteration=model.best_iteration)\n",
    "    \n",
    "    print(train_y.max(), val_y.max())\n",
    "    \n",
    "    return pred_test_y, pred_oof_log, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_calculations(X, test, big_cv_folds, func_name = None):\n",
    "    if not func_name:\n",
    "        return print('The function to run is not defined')\n",
    "    else:\n",
    "        y_oof_20_preds = []\n",
    "        fold_errors_20_preds =[]\n",
    "        avg_test_pred_20_preds = []\n",
    "        models_20_preds = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for ind, cv_folds in enumerate(big_cv_folds):\n",
    "#             print('Fitting big fold', ind+1, 'out of', len(big_cv_folds))\n",
    "            y_oof = np.zeros((y.shape[0]))\n",
    "            fold_errors =[]\n",
    "            pred_test_list = []\n",
    "            models = []\n",
    "            \n",
    "            for i, (train_index, val_index) in enumerate(cv_folds):\n",
    "                print('Fitting big fold', ind+1, 'out of', len(big_cv_folds),\n",
    "                      'and sub fold', i+1, 'out of', len(cv_folds))\n",
    "#                 X_train, X_val  = X[ind][train_index], X[ind][val_index]\n",
    "#                 y_train, y_val = y[train_index], y[val_index]\n",
    "                X_train, X_val  = X.iloc[train_index], X.iloc[val_index]\n",
    "                y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "                # part to include additional functions\n",
    "                if func_name == 'lgb':\n",
    "#                     pred_test_y, pred_oof_log, clf = run_model(X_train, y_train, X_val, y_val, test[ind],\n",
    "#                                                              seed = (RANDOM_STATE + ind*i))\n",
    "                    pred_test_y, pred_oof_log, clf = run_model(X_train, y_train, X_val, y_val, test,\n",
    "                                                             seed = (RANDOM_STATE + ind*i))\n",
    "                    models.append(clf)\n",
    "                else:\n",
    "                    return print('The function to run is not correct')\n",
    "\n",
    "                y_oof[val_index] = pred_oof_log\n",
    "                curr_fe = np.sqrt(mean_squared_error(y_val, pred_oof_log))\n",
    "                print(f'Fold error {curr_fe}')\n",
    "                fold_errors.append(curr_fe)\n",
    "                pred_test_list.append(list(pred_test_y))\n",
    "                print('Time passed: {} seconds.'.format(time.time() - start_time))\n",
    "\n",
    "            print('Average big fold', ind+1, 'error:', np.sqrt(mean_squared_error(y, y_oof)))\n",
    "            total_fe_std = round(np.std(fold_errors), 5)\n",
    "            print(f'Total big fold {ind+1} std {total_fe_std}')\n",
    "            avg_test_pred = np.mean(pred_test_list, axis=0)\n",
    "            \n",
    "            avg_test_pred_20_preds.append(avg_test_pred)\n",
    "            fold_errors_20_preds.append(fold_errors)\n",
    "            y_oof_20_preds.append(y_oof)\n",
    "            models_20_preds.append(models)\n",
    "            \n",
    "    return y_oof_20_preds, avg_test_pred_20_preds, fold_errors_20_preds, models_20_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_evaluate(max_depth,\n",
    "                 num_leaves,\n",
    "                 bagging_fraction,\n",
    "                 feature_fraction,\n",
    "                 bagging_freq):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        # \"num_leaves\": 361,  # 40\n",
    "        # 'max_depth': 21,\n",
    "        \"learning_rate\": 0.04,  # 0.005\n",
    "        # \"bagging_fraction\": 0.7,\n",
    "        # \"feature_fraction\": 0.4,  # 0.6\n",
    "        # \"bagging_freq\": 5,\n",
    "        \"verbosity\": -1,\n",
    "        'num_threads': 4,\n",
    "        \"seed\": random_state\n",
    "    }\n",
    "\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['num_leaves'] = int(num_leaves)\n",
    "    params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "    params['feature_fraction'] = max(feature_fraction, 0)\n",
    "    params['bagging_freq'] = int(bagging_freq)\n",
    "\n",
    "    cv_result = lgb.cv(params, lgtrain, num_boost_round=num_rounds, nfold=5,\n",
    "                       seed=random_state,\n",
    "                       verbose_eval=20,\n",
    "                       stratified=False, #have to add, because of objective regression\n",
    "                       early_stopping_rounds=50)\n",
    "\n",
    "    return -cv_result['rmse-mean'][-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgtrain = lgb.Dataset(X, label=y)\n",
    "\n",
    "num_rounds = 100\n",
    "random_state = 2018\n",
    "num_iter = 50\n",
    "init_points = 8\n",
    "# params = {\n",
    "#     'eta': 0.1,\n",
    "#     'silent': 1,\n",
    "#     'eval_metric': 'mae',\n",
    "#     'verbose_eval': True,\n",
    "#     'seed': random_state\n",
    "# }\n",
    "\n",
    "lgbBO = BayesianOptimization(lgb_evaluate, {'max_depth': (5, 23),\n",
    "                                            'num_leaves': (150, 2048),\n",
    "                                            'bagging_fraction': (0.4, 0.9),\n",
    "                                            'feature_fraction': (0.8, 0.99),\n",
    "                                            'bagging_freq': (1, 4),\n",
    "                                            })\n",
    "lgbBO.explore({'max_depth': [8, 5, 8, 13, 21],\n",
    "               'num_leaves': [188, 40, 80, 361, 512],\n",
    "               'bagging_fraction': [0.7417, 0.9, 0.5, 0.6, 0.4],\n",
    "               'feature_fraction': [0.9884, 0.8, 0.99, 0.8, 0.99],\n",
    "               'bagging_freq': [2, 1, 2, 2, 1],\n",
    "               })\n",
    "lgbBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "# Finally, we take a look at the final results.\n",
    "print(lgbBO.res['max'])\n",
    "print(lgbBO.res['all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting big fold 1 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31158\tvalid_1's rmse: 1.38838\n",
      "[400]\ttraining's rmse: 1.15203\tvalid_1's rmse: 1.3005\n",
      "[600]\ttraining's rmse: 1.07669\tvalid_1's rmse: 1.28113\n",
      "[800]\ttraining's rmse: 1.02286\tvalid_1's rmse: 1.27589\n",
      "[1000]\ttraining's rmse: 0.981258\tvalid_1's rmse: 1.27391\n",
      "[1200]\ttraining's rmse: 0.940506\tvalid_1's rmse: 1.27325\n",
      "[1400]\ttraining's rmse: 0.904548\tvalid_1's rmse: 1.27285\n",
      "[1600]\ttraining's rmse: 0.869702\tvalid_1's rmse: 1.2737\n",
      "Early stopping, best iteration is:\n",
      "[1439]\ttraining's rmse: 0.897725\tvalid_1's rmse: 1.27278\n",
      "Model training done in 23.80232048034668 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2727760092726583\n",
      "Time passed: 26.230388641357422 seconds.\n",
      "Fitting big fold 1 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33192\tvalid_1's rmse: 1.35333\n",
      "[400]\ttraining's rmse: 1.18836\tvalid_1's rmse: 1.25669\n",
      "[600]\ttraining's rmse: 1.12222\tvalid_1's rmse: 1.23591\n",
      "[800]\ttraining's rmse: 1.078\tvalid_1's rmse: 1.23114\n",
      "[1000]\ttraining's rmse: 1.04146\tvalid_1's rmse: 1.23002\n",
      "[1200]\ttraining's rmse: 1.00594\tvalid_1's rmse: 1.22974\n",
      "[1400]\ttraining's rmse: 0.974278\tvalid_1's rmse: 1.22934\n",
      "[1600]\ttraining's rmse: 0.94386\tvalid_1's rmse: 1.2291\n",
      "Early stopping, best iteration is:\n",
      "[1433]\ttraining's rmse: 0.969379\tvalid_1's rmse: 1.22873\n",
      "Model training done in 27.618475675582886 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2287331219572728\n",
      "Time passed: 56.20243287086487 seconds.\n",
      "Fitting big fold 1 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32842\tvalid_1's rmse: 1.37209\n",
      "[400]\ttraining's rmse: 1.18368\tvalid_1's rmse: 1.28637\n",
      "[600]\ttraining's rmse: 1.11849\tvalid_1's rmse: 1.26818\n",
      "[800]\ttraining's rmse: 1.07626\tvalid_1's rmse: 1.26424\n",
      "[1000]\ttraining's rmse: 1.03822\tvalid_1's rmse: 1.26286\n",
      "Early stopping, best iteration is:\n",
      "[914]\ttraining's rmse: 1.05431\tvalid_1's rmse: 1.26257\n",
      "Model training done in 18.851024866104126 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2625653195444901\n",
      "Time passed: 76.85397410392761 seconds.\n",
      "Fitting big fold 1 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31901\tvalid_1's rmse: 1.43099\n",
      "[400]\ttraining's rmse: 1.17406\tvalid_1's rmse: 1.34199\n",
      "[600]\ttraining's rmse: 1.10933\tvalid_1's rmse: 1.31934\n",
      "[800]\ttraining's rmse: 1.06477\tvalid_1's rmse: 1.31238\n",
      "[1000]\ttraining's rmse: 1.0285\tvalid_1's rmse: 1.30994\n",
      "[1200]\ttraining's rmse: 0.994304\tvalid_1's rmse: 1.30985\n",
      "[1400]\ttraining's rmse: 0.962314\tvalid_1's rmse: 1.31008\n",
      "Early stopping, best iteration is:\n",
      "[1261]\ttraining's rmse: 0.9842\tvalid_1's rmse: 1.3091\n",
      "Model training done in 24.99887251853943 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3090953381072146\n",
      "Time passed: 103.98974299430847 seconds.\n",
      "Fitting big fold 1 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32652\tvalid_1's rmse: 1.38092\n",
      "[400]\ttraining's rmse: 1.1803\tvalid_1's rmse: 1.29542\n",
      "[600]\ttraining's rmse: 1.11259\tvalid_1's rmse: 1.27527\n",
      "[800]\ttraining's rmse: 1.06847\tvalid_1's rmse: 1.2694\n",
      "[1000]\ttraining's rmse: 1.03051\tvalid_1's rmse: 1.26795\n",
      "[1200]\ttraining's rmse: 0.994344\tvalid_1's rmse: 1.26669\n",
      "[1400]\ttraining's rmse: 0.961349\tvalid_1's rmse: 1.26595\n",
      "[1600]\ttraining's rmse: 0.928998\tvalid_1's rmse: 1.26497\n",
      "[1800]\ttraining's rmse: 0.899431\tvalid_1's rmse: 1.26469\n",
      "[2000]\ttraining's rmse: 0.870818\tvalid_1's rmse: 1.26405\n",
      "[2200]\ttraining's rmse: 0.845089\tvalid_1's rmse: 1.26316\n",
      "[2400]\ttraining's rmse: 0.819158\tvalid_1's rmse: 1.2626\n",
      "[2600]\ttraining's rmse: 0.794247\tvalid_1's rmse: 1.26274\n",
      "Early stopping, best iteration is:\n",
      "[2483]\ttraining's rmse: 0.809094\tvalid_1's rmse: 1.26234\n",
      "Model training done in 42.41991567611694 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2623411029574911\n",
      "Time passed: 150.02816224098206 seconds.\n",
      "Average big fold 1 error: 1.2709320387523888\n",
      "Total big fold 1 std 0.02573\n",
      "Fitting big fold 2 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.29978\tvalid_1's rmse: 1.41278\n",
      "[400]\ttraining's rmse: 1.13758\tvalid_1's rmse: 1.33238\n",
      "[600]\ttraining's rmse: 1.06152\tvalid_1's rmse: 1.3137\n",
      "[800]\ttraining's rmse: 1.01048\tvalid_1's rmse: 1.30918\n",
      "[1000]\ttraining's rmse: 0.967339\tvalid_1's rmse: 1.30839\n",
      "[1200]\ttraining's rmse: 0.925259\tvalid_1's rmse: 1.30887\n",
      "Early stopping, best iteration is:\n",
      "[1062]\ttraining's rmse: 0.954053\tvalid_1's rmse: 1.30785\n",
      "Model training done in 20.236037969589233 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3078488300716078\n",
      "Time passed: 172.47969770431519 seconds.\n",
      "Fitting big fold 2 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33476\tvalid_1's rmse: 1.34319\n",
      "[400]\ttraining's rmse: 1.19083\tvalid_1's rmse: 1.24347\n",
      "[600]\ttraining's rmse: 1.12527\tvalid_1's rmse: 1.2216\n",
      "[800]\ttraining's rmse: 1.08162\tvalid_1's rmse: 1.21667\n",
      "[1000]\ttraining's rmse: 1.04546\tvalid_1's rmse: 1.21678\n",
      "Early stopping, best iteration is:\n",
      "[861]\ttraining's rmse: 1.07029\tvalid_1's rmse: 1.21586\n",
      "Model training done in 20.309032201766968 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.215862065103941\n",
      "Time passed: 194.81873226165771 seconds.\n",
      "Fitting big fold 2 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33094\tvalid_1's rmse: 1.35524\n",
      "[400]\ttraining's rmse: 1.18858\tvalid_1's rmse: 1.25092\n",
      "[600]\ttraining's rmse: 1.12296\tvalid_1's rmse: 1.22394\n",
      "[800]\ttraining's rmse: 1.08037\tvalid_1's rmse: 1.21531\n",
      "[1000]\ttraining's rmse: 1.04351\tvalid_1's rmse: 1.21335\n",
      "[1200]\ttraining's rmse: 1.00907\tvalid_1's rmse: 1.21193\n",
      "[1400]\ttraining's rmse: 0.978189\tvalid_1's rmse: 1.2101\n",
      "Early stopping, best iteration is:\n",
      "[1326]\ttraining's rmse: 0.989501\tvalid_1's rmse: 1.20977\n",
      "Model training done in 26.612465381622314 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2097670603698794\n",
      "Time passed: 223.67744326591492 seconds.\n",
      "Fitting big fold 2 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32066\tvalid_1's rmse: 1.4119\n",
      "[400]\ttraining's rmse: 1.17629\tvalid_1's rmse: 1.31926\n",
      "[600]\ttraining's rmse: 1.10931\tvalid_1's rmse: 1.29823\n",
      "[800]\ttraining's rmse: 1.06549\tvalid_1's rmse: 1.29383\n",
      "[1000]\ttraining's rmse: 1.03076\tvalid_1's rmse: 1.29375\n",
      "Early stopping, best iteration is:\n",
      "[853]\ttraining's rmse: 1.05531\tvalid_1's rmse: 1.29281\n",
      "Model training done in 17.439504384994507 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.29280840994266\n",
      "Time passed: 242.9774467945099 seconds.\n",
      "Fitting big fold 2 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32547\tvalid_1's rmse: 1.36092\n",
      "[400]\ttraining's rmse: 1.17944\tvalid_1's rmse: 1.27905\n",
      "[600]\ttraining's rmse: 1.11228\tvalid_1's rmse: 1.26357\n",
      "[800]\ttraining's rmse: 1.06639\tvalid_1's rmse: 1.26082\n",
      "[1000]\ttraining's rmse: 1.02662\tvalid_1's rmse: 1.25872\n",
      "[1200]\ttraining's rmse: 0.991013\tvalid_1's rmse: 1.25719\n",
      "[1400]\ttraining's rmse: 0.956289\tvalid_1's rmse: 1.25603\n",
      "[1600]\ttraining's rmse: 0.924274\tvalid_1's rmse: 1.25623\n",
      "[1800]\ttraining's rmse: 0.894231\tvalid_1's rmse: 1.25547\n",
      "[2000]\ttraining's rmse: 0.864867\tvalid_1's rmse: 1.25551\n",
      "Early stopping, best iteration is:\n",
      "[1816]\ttraining's rmse: 0.891839\tvalid_1's rmse: 1.25491\n",
      "Model training done in 32.23231840133667 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2549117547400195\n",
      "Time passed: 277.9700484275818 seconds.\n",
      "Average big fold 2 error: 1.2734701851476256\n",
      "Total big fold 2 std 0.03948\n",
      "Fitting big fold 3 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31687\tvalid_1's rmse: 1.37838\n",
      "[400]\ttraining's rmse: 1.15657\tvalid_1's rmse: 1.29402\n",
      "[600]\ttraining's rmse: 1.07911\tvalid_1's rmse: 1.27688\n",
      "[800]\ttraining's rmse: 1.02453\tvalid_1's rmse: 1.27354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's rmse: 0.977842\tvalid_1's rmse: 1.27376\n",
      "Early stopping, best iteration is:\n",
      "[908]\ttraining's rmse: 0.998717\tvalid_1's rmse: 1.27306\n",
      "Model training done in 18.94931173324585 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2730641738447022\n",
      "Time passed: 299.0283226966858 seconds.\n",
      "Fitting big fold 3 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33357\tvalid_1's rmse: 1.35366\n",
      "[400]\ttraining's rmse: 1.19179\tvalid_1's rmse: 1.25127\n",
      "[600]\ttraining's rmse: 1.12708\tvalid_1's rmse: 1.22444\n",
      "[800]\ttraining's rmse: 1.08278\tvalid_1's rmse: 1.2145\n",
      "[1000]\ttraining's rmse: 1.04618\tvalid_1's rmse: 1.21231\n",
      "[1200]\ttraining's rmse: 1.01169\tvalid_1's rmse: 1.20992\n",
      "[1400]\ttraining's rmse: 0.977366\tvalid_1's rmse: 1.21028\n",
      "Early stopping, best iteration is:\n",
      "[1225]\ttraining's rmse: 1.00759\tvalid_1's rmse: 1.20965\n",
      "Model training done in 26.107792854309082 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2096495487281245\n",
      "Time passed: 327.2697093486786 seconds.\n",
      "Fitting big fold 3 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32443\tvalid_1's rmse: 1.40467\n",
      "[400]\ttraining's rmse: 1.18125\tvalid_1's rmse: 1.31552\n",
      "[600]\ttraining's rmse: 1.1157\tvalid_1's rmse: 1.29739\n",
      "[800]\ttraining's rmse: 1.07181\tvalid_1's rmse: 1.29391\n",
      "[1000]\ttraining's rmse: 1.03515\tvalid_1's rmse: 1.29333\n",
      "[1200]\ttraining's rmse: 1.0009\tvalid_1's rmse: 1.29235\n",
      "[1400]\ttraining's rmse: 0.967896\tvalid_1's rmse: 1.29061\n",
      "[1600]\ttraining's rmse: 0.936557\tvalid_1's rmse: 1.29003\n",
      "Early stopping, best iteration is:\n",
      "[1589]\ttraining's rmse: 0.938374\tvalid_1's rmse: 1.28982\n",
      "Model training done in 28.584035634994507 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2898240386080728\n",
      "Time passed: 358.3767457008362 seconds.\n",
      "Fitting big fold 3 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32666\tvalid_1's rmse: 1.38986\n",
      "[400]\ttraining's rmse: 1.17897\tvalid_1's rmse: 1.30184\n",
      "[600]\ttraining's rmse: 1.1123\tvalid_1's rmse: 1.28029\n",
      "[800]\ttraining's rmse: 1.06851\tvalid_1's rmse: 1.27496\n",
      "[1000]\ttraining's rmse: 1.0314\tvalid_1's rmse: 1.27218\n",
      "[1200]\ttraining's rmse: 0.997999\tvalid_1's rmse: 1.27101\n",
      "[1400]\ttraining's rmse: 0.966566\tvalid_1's rmse: 1.27\n",
      "[1600]\ttraining's rmse: 0.936424\tvalid_1's rmse: 1.26995\n",
      "[1800]\ttraining's rmse: 0.906588\tvalid_1's rmse: 1.26961\n",
      "[2000]\ttraining's rmse: 0.877843\tvalid_1's rmse: 1.26976\n",
      "Early stopping, best iteration is:\n",
      "[1820]\ttraining's rmse: 0.903467\tvalid_1's rmse: 1.26911\n",
      "Model training done in 33.62196731567383 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.269111149190434\n",
      "Time passed: 394.81281447410583 seconds.\n",
      "Fitting big fold 3 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31672\tvalid_1's rmse: 1.41591\n",
      "[400]\ttraining's rmse: 1.16973\tvalid_1's rmse: 1.33193\n",
      "[600]\ttraining's rmse: 1.10154\tvalid_1's rmse: 1.31283\n",
      "[800]\ttraining's rmse: 1.0565\tvalid_1's rmse: 1.30652\n",
      "[1000]\ttraining's rmse: 1.01741\tvalid_1's rmse: 1.30706\n",
      "Early stopping, best iteration is:\n",
      "[877]\ttraining's rmse: 1.04077\tvalid_1's rmse: 1.30574\n",
      "Model training done in 17.7254855632782 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3057419523525726\n",
      "Time passed: 414.38632678985596 seconds.\n",
      "Average big fold 3 error: 1.2747749378526845\n",
      "Total big fold 3 std 0.03262\n",
      "Fitting big fold 4 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31661\tvalid_1's rmse: 1.38047\n",
      "[400]\ttraining's rmse: 1.16027\tvalid_1's rmse: 1.29448\n",
      "[600]\ttraining's rmse: 1.08548\tvalid_1's rmse: 1.27651\n",
      "[800]\ttraining's rmse: 1.03343\tvalid_1's rmse: 1.27375\n",
      "[1000]\ttraining's rmse: 0.989726\tvalid_1's rmse: 1.27293\n",
      "Early stopping, best iteration is:\n",
      "[975]\ttraining's rmse: 0.99516\tvalid_1's rmse: 1.27268\n",
      "Model training done in 17.50960373878479 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.272682333503447\n",
      "Time passed: 433.84444880485535 seconds.\n",
      "Fitting big fold 4 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32888\tvalid_1's rmse: 1.38582\n",
      "[400]\ttraining's rmse: 1.18553\tvalid_1's rmse: 1.29301\n",
      "[600]\ttraining's rmse: 1.12018\tvalid_1's rmse: 1.27019\n",
      "[800]\ttraining's rmse: 1.07647\tvalid_1's rmse: 1.26382\n",
      "[1000]\ttraining's rmse: 1.03901\tvalid_1's rmse: 1.26138\n",
      "[1200]\ttraining's rmse: 1.00506\tvalid_1's rmse: 1.25904\n",
      "[1400]\ttraining's rmse: 0.972509\tvalid_1's rmse: 1.25827\n",
      "[1600]\ttraining's rmse: 0.943465\tvalid_1's rmse: 1.25746\n",
      "[1800]\ttraining's rmse: 0.914971\tvalid_1's rmse: 1.2574\n",
      "Early stopping, best iteration is:\n",
      "[1746]\ttraining's rmse: 0.922619\tvalid_1's rmse: 1.2569\n",
      "Model training done in 31.071959018707275 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2569049766136124\n",
      "Time passed: 467.56040835380554 seconds.\n",
      "Fitting big fold 4 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32566\tvalid_1's rmse: 1.40872\n",
      "[400]\ttraining's rmse: 1.18133\tvalid_1's rmse: 1.31317\n",
      "[600]\ttraining's rmse: 1.11576\tvalid_1's rmse: 1.28935\n",
      "[800]\ttraining's rmse: 1.07146\tvalid_1's rmse: 1.28183\n",
      "[1000]\ttraining's rmse: 1.03479\tvalid_1's rmse: 1.27982\n",
      "[1200]\ttraining's rmse: 0.99981\tvalid_1's rmse: 1.27879\n",
      "[1400]\ttraining's rmse: 0.967211\tvalid_1's rmse: 1.27854\n",
      "Early stopping, best iteration is:\n",
      "[1294]\ttraining's rmse: 0.984725\tvalid_1's rmse: 1.27838\n",
      "Model training done in 24.29262685775757 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2783768747680935\n",
      "Time passed: 494.0605342388153 seconds.\n",
      "Fitting big fold 4 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32493\tvalid_1's rmse: 1.39156\n",
      "[400]\ttraining's rmse: 1.17865\tvalid_1's rmse: 1.30924\n",
      "[600]\ttraining's rmse: 1.11118\tvalid_1's rmse: 1.29062\n",
      "[800]\ttraining's rmse: 1.06715\tvalid_1's rmse: 1.28664\n",
      "[1000]\ttraining's rmse: 1.02969\tvalid_1's rmse: 1.28437\n",
      "[1200]\ttraining's rmse: 0.995562\tvalid_1's rmse: 1.28359\n",
      "[1400]\ttraining's rmse: 0.962921\tvalid_1's rmse: 1.28228\n",
      "[1600]\ttraining's rmse: 0.931738\tvalid_1's rmse: 1.2816\n",
      "[1800]\ttraining's rmse: 0.901806\tvalid_1's rmse: 1.2795\n",
      "[2000]\ttraining's rmse: 0.874099\tvalid_1's rmse: 1.27914\n",
      "[2200]\ttraining's rmse: 0.847807\tvalid_1's rmse: 1.27804\n",
      "[2400]\ttraining's rmse: 0.822786\tvalid_1's rmse: 1.27849\n",
      "Early stopping, best iteration is:\n",
      "[2200]\ttraining's rmse: 0.847807\tvalid_1's rmse: 1.27804\n",
      "Model training done in 37.34338450431824 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2780413611848742\n",
      "Time passed: 534.4239175319672 seconds.\n",
      "Fitting big fold 4 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32198\tvalid_1's rmse: 1.38871\n",
      "[400]\ttraining's rmse: 1.17277\tvalid_1's rmse: 1.30419\n",
      "[600]\ttraining's rmse: 1.10322\tvalid_1's rmse: 1.28553\n",
      "[800]\ttraining's rmse: 1.05808\tvalid_1's rmse: 1.28108\n",
      "[1000]\ttraining's rmse: 1.02075\tvalid_1's rmse: 1.28044\n",
      "[1200]\ttraining's rmse: 0.986928\tvalid_1's rmse: 1.28008\n",
      "[1400]\ttraining's rmse: 0.95385\tvalid_1's rmse: 1.28045\n",
      "Early stopping, best iteration is:\n",
      "[1245]\ttraining's rmse: 0.979177\tvalid_1's rmse: 1.2795\n",
      "Model training done in 22.838042974472046 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2794991913112395\n",
      "Time passed: 559.4622278213501 seconds.\n",
      "Average big fold 4 error: 1.2740954769791424\n",
      "Total big fold 4 std 0.00843\n",
      "Fitting big fold 5 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31322\tvalid_1's rmse: 1.39054\n",
      "[400]\ttraining's rmse: 1.15656\tvalid_1's rmse: 1.30109\n",
      "[600]\ttraining's rmse: 1.08111\tvalid_1's rmse: 1.27973\n",
      "[800]\ttraining's rmse: 1.03029\tvalid_1's rmse: 1.27366\n",
      "[1000]\ttraining's rmse: 0.988135\tvalid_1's rmse: 1.27072\n",
      "[1200]\ttraining's rmse: 0.948144\tvalid_1's rmse: 1.27007\n",
      "[1400]\ttraining's rmse: 0.912345\tvalid_1's rmse: 1.26999\n",
      "Early stopping, best iteration is:\n",
      "[1236]\ttraining's rmse: 0.941411\tvalid_1's rmse: 1.26981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done in 20.732223987579346 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2698104139500026\n",
      "Time passed: 582.3625359535217 seconds.\n",
      "Fitting big fold 5 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33253\tvalid_1's rmse: 1.36357\n",
      "[400]\ttraining's rmse: 1.18889\tvalid_1's rmse: 1.26786\n",
      "[600]\ttraining's rmse: 1.12251\tvalid_1's rmse: 1.24406\n",
      "[800]\ttraining's rmse: 1.0793\tvalid_1's rmse: 1.23914\n",
      "[1000]\ttraining's rmse: 1.04298\tvalid_1's rmse: 1.2394\n",
      "Early stopping, best iteration is:\n",
      "[883]\ttraining's rmse: 1.06448\tvalid_1's rmse: 1.23833\n",
      "Model training done in 18.341240406036377 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2383254710806757\n",
      "Time passed: 602.4843242168427 seconds.\n",
      "Fitting big fold 5 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32962\tvalid_1's rmse: 1.36744\n",
      "[400]\ttraining's rmse: 1.1851\tvalid_1's rmse: 1.27854\n",
      "[600]\ttraining's rmse: 1.11931\tvalid_1's rmse: 1.26225\n",
      "[800]\ttraining's rmse: 1.07453\tvalid_1's rmse: 1.25832\n",
      "[1000]\ttraining's rmse: 1.038\tvalid_1's rmse: 1.25621\n",
      "[1200]\ttraining's rmse: 1.00288\tvalid_1's rmse: 1.2561\n",
      "[1400]\ttraining's rmse: 0.970304\tvalid_1's rmse: 1.25387\n",
      "[1600]\ttraining's rmse: 0.940776\tvalid_1's rmse: 1.25348\n",
      "[1800]\ttraining's rmse: 0.911322\tvalid_1's rmse: 1.2541\n",
      "Early stopping, best iteration is:\n",
      "[1717]\ttraining's rmse: 0.922629\tvalid_1's rmse: 1.25268\n",
      "Model training done in 30.238256216049194 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2526797658942788\n",
      "Time passed: 635.3509798049927 seconds.\n",
      "Fitting big fold 5 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31612\tvalid_1's rmse: 1.42682\n",
      "[400]\ttraining's rmse: 1.16725\tvalid_1's rmse: 1.34989\n",
      "[600]\ttraining's rmse: 1.10069\tvalid_1's rmse: 1.33641\n",
      "[800]\ttraining's rmse: 1.05585\tvalid_1's rmse: 1.33437\n",
      "[1000]\ttraining's rmse: 1.01893\tvalid_1's rmse: 1.33422\n",
      "Early stopping, best iteration is:\n",
      "[981]\ttraining's rmse: 1.02237\tvalid_1's rmse: 1.33368\n",
      "Model training done in 19.73019814491272 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3336766488676715\n",
      "Time passed: 657.175793170929 seconds.\n",
      "Fitting big fold 5 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3271\tvalid_1's rmse: 1.37307\n",
      "[400]\ttraining's rmse: 1.18131\tvalid_1's rmse: 1.28017\n",
      "[600]\ttraining's rmse: 1.11381\tvalid_1's rmse: 1.25831\n",
      "[800]\ttraining's rmse: 1.07032\tvalid_1's rmse: 1.25183\n",
      "[1000]\ttraining's rmse: 1.03228\tvalid_1's rmse: 1.24995\n",
      "[1200]\ttraining's rmse: 0.995458\tvalid_1's rmse: 1.24861\n",
      "[1400]\ttraining's rmse: 0.960844\tvalid_1's rmse: 1.24848\n",
      "[1600]\ttraining's rmse: 0.930062\tvalid_1's rmse: 1.24813\n",
      "[1800]\ttraining's rmse: 0.900304\tvalid_1's rmse: 1.24787\n",
      "Early stopping, best iteration is:\n",
      "[1700]\ttraining's rmse: 0.915158\tvalid_1's rmse: 1.24745\n",
      "Model training done in 33.751300573349 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.247454716480669\n",
      "Time passed: 693.728199005127 seconds.\n",
      "Average big fold 5 error: 1.2707988456889336\n",
      "Total big fold 5 std 0.03422\n",
      "Fitting big fold 6 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.30904\tvalid_1's rmse: 1.38736\n",
      "[400]\ttraining's rmse: 1.15017\tvalid_1's rmse: 1.30528\n",
      "[600]\ttraining's rmse: 1.07625\tvalid_1's rmse: 1.28899\n",
      "[800]\ttraining's rmse: 1.02384\tvalid_1's rmse: 1.28487\n",
      "[1000]\ttraining's rmse: 0.981367\tvalid_1's rmse: 1.28386\n",
      "Early stopping, best iteration is:\n",
      "[912]\ttraining's rmse: 0.999747\tvalid_1's rmse: 1.2836\n",
      "Model training done in 20.21958637237549 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2835996121031417\n",
      "Time passed: 715.9170928001404 seconds.\n",
      "Fitting big fold 6 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33012\tvalid_1's rmse: 1.37739\n",
      "[400]\ttraining's rmse: 1.18779\tvalid_1's rmse: 1.28239\n",
      "[600]\ttraining's rmse: 1.12375\tvalid_1's rmse: 1.26119\n",
      "[800]\ttraining's rmse: 1.08042\tvalid_1's rmse: 1.25504\n",
      "[1000]\ttraining's rmse: 1.0435\tvalid_1's rmse: 1.25369\n",
      "[1200]\ttraining's rmse: 1.00882\tvalid_1's rmse: 1.2529\n",
      "[1400]\ttraining's rmse: 0.976406\tvalid_1's rmse: 1.25248\n",
      "[1600]\ttraining's rmse: 0.946162\tvalid_1's rmse: 1.25232\n",
      "Early stopping, best iteration is:\n",
      "[1500]\ttraining's rmse: 0.96127\tvalid_1's rmse: 1.25147\n",
      "Model training done in 30.06625485420227 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.251467337648126\n",
      "Time passed: 748.50315117836 seconds.\n",
      "Fitting big fold 6 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33149\tvalid_1's rmse: 1.36819\n",
      "[400]\ttraining's rmse: 1.18833\tvalid_1's rmse: 1.26682\n",
      "[600]\ttraining's rmse: 1.12261\tvalid_1's rmse: 1.23922\n",
      "[800]\ttraining's rmse: 1.07637\tvalid_1's rmse: 1.2306\n",
      "[1000]\ttraining's rmse: 1.03818\tvalid_1's rmse: 1.22943\n",
      "Early stopping, best iteration is:\n",
      "[920]\ttraining's rmse: 1.05295\tvalid_1's rmse: 1.22863\n",
      "Model training done in 21.80231523513794 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2286347936305093\n",
      "Time passed: 772.8020470142365 seconds.\n",
      "Fitting big fold 6 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3288\tvalid_1's rmse: 1.3703\n",
      "[400]\ttraining's rmse: 1.18416\tvalid_1's rmse: 1.27793\n",
      "[600]\ttraining's rmse: 1.1173\tvalid_1's rmse: 1.25726\n",
      "[800]\ttraining's rmse: 1.07201\tvalid_1's rmse: 1.25221\n",
      "[1000]\ttraining's rmse: 1.03523\tvalid_1's rmse: 1.2503\n",
      "[1200]\ttraining's rmse: 0.999658\tvalid_1's rmse: 1.24984\n",
      "[1400]\ttraining's rmse: 0.966626\tvalid_1's rmse: 1.24954\n",
      "Early stopping, best iteration is:\n",
      "[1253]\ttraining's rmse: 0.990945\tvalid_1's rmse: 1.24924\n",
      "Model training done in 29.798630952835083 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.249243259566921\n",
      "Time passed: 804.9451806545258 seconds.\n",
      "Fitting big fold 6 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31639\tvalid_1's rmse: 1.41103\n",
      "[400]\ttraining's rmse: 1.17003\tvalid_1's rmse: 1.32748\n",
      "[600]\ttraining's rmse: 1.10273\tvalid_1's rmse: 1.3092\n",
      "[800]\ttraining's rmse: 1.05682\tvalid_1's rmse: 1.30518\n",
      "[1000]\ttraining's rmse: 1.01886\tvalid_1's rmse: 1.30414\n",
      "Early stopping, best iteration is:\n",
      "[925]\ttraining's rmse: 1.03255\tvalid_1's rmse: 1.30371\n",
      "Model training done in 20.9036545753479 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3037143454369586\n",
      "Time passed: 827.7128338813782 seconds.\n",
      "Average big fold 6 error: 1.2718231327066976\n",
      "Total big fold 6 std 0.02678\n",
      "Fitting big fold 7 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31721\tvalid_1's rmse: 1.37674\n",
      "[400]\ttraining's rmse: 1.16039\tvalid_1's rmse: 1.29065\n",
      "[600]\ttraining's rmse: 1.08355\tvalid_1's rmse: 1.27112\n",
      "[800]\ttraining's rmse: 1.03119\tvalid_1's rmse: 1.26719\n",
      "[1000]\ttraining's rmse: 0.986923\tvalid_1's rmse: 1.26646\n",
      "[1200]\ttraining's rmse: 0.945604\tvalid_1's rmse: 1.26711\n",
      "Early stopping, best iteration is:\n",
      "[1006]\ttraining's rmse: 0.985728\tvalid_1's rmse: 1.26643\n",
      "Model training done in 18.379066467285156 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2664312529481712\n",
      "Time passed: 848.5552279949188 seconds.\n",
      "Fitting big fold 7 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32857\tvalid_1's rmse: 1.40146\n",
      "[400]\ttraining's rmse: 1.18718\tvalid_1's rmse: 1.29818\n",
      "[600]\ttraining's rmse: 1.12217\tvalid_1's rmse: 1.26595\n",
      "[800]\ttraining's rmse: 1.07857\tvalid_1's rmse: 1.25411\n",
      "[1000]\ttraining's rmse: 1.04306\tvalid_1's rmse: 1.24834\n",
      "[1200]\ttraining's rmse: 1.00809\tvalid_1's rmse: 1.24886\n",
      "Early stopping, best iteration is:\n",
      "[1100]\ttraining's rmse: 1.02556\tvalid_1's rmse: 1.24767\n",
      "Model training done in 22.697769165039062 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.247674913137701\n",
      "Time passed: 873.4341924190521 seconds.\n",
      "Fitting big fold 7 out of 20 and sub fold 3 out of 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3322\tvalid_1's rmse: 1.32651\n",
      "[400]\ttraining's rmse: 1.18898\tvalid_1's rmse: 1.23843\n",
      "[600]\ttraining's rmse: 1.12452\tvalid_1's rmse: 1.22404\n",
      "[800]\ttraining's rmse: 1.08065\tvalid_1's rmse: 1.22291\n",
      "[1000]\ttraining's rmse: 1.04268\tvalid_1's rmse: 1.22387\n",
      "Early stopping, best iteration is:\n",
      "[840]\ttraining's rmse: 1.07314\tvalid_1's rmse: 1.22242\n",
      "Model training done in 20.937936305999756 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2224223788574695\n",
      "Time passed: 896.2828195095062 seconds.\n",
      "Fitting big fold 7 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32365\tvalid_1's rmse: 1.40358\n",
      "[400]\ttraining's rmse: 1.17908\tvalid_1's rmse: 1.31901\n",
      "[600]\ttraining's rmse: 1.11329\tvalid_1's rmse: 1.2989\n",
      "[800]\ttraining's rmse: 1.06853\tvalid_1's rmse: 1.29168\n",
      "[1000]\ttraining's rmse: 1.02989\tvalid_1's rmse: 1.28741\n",
      "[1200]\ttraining's rmse: 0.995394\tvalid_1's rmse: 1.28508\n",
      "[1400]\ttraining's rmse: 0.962343\tvalid_1's rmse: 1.28278\n",
      "[1600]\ttraining's rmse: 0.931948\tvalid_1's rmse: 1.28184\n",
      "[1800]\ttraining's rmse: 0.902314\tvalid_1's rmse: 1.28051\n",
      "[2000]\ttraining's rmse: 0.875002\tvalid_1's rmse: 1.28011\n",
      "Early stopping, best iteration is:\n",
      "[1898]\ttraining's rmse: 0.889075\tvalid_1's rmse: 1.27945\n",
      "Model training done in 34.965266704559326 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2794528698536525\n",
      "Time passed: 934.0768837928772 seconds.\n",
      "Fitting big fold 7 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3142\tvalid_1's rmse: 1.42279\n",
      "[400]\ttraining's rmse: 1.16538\tvalid_1's rmse: 1.34167\n",
      "[600]\ttraining's rmse: 1.09747\tvalid_1's rmse: 1.32356\n",
      "[800]\ttraining's rmse: 1.05262\tvalid_1's rmse: 1.3202\n",
      "[1000]\ttraining's rmse: 1.0152\tvalid_1's rmse: 1.31951\n",
      "Early stopping, best iteration is:\n",
      "[963]\ttraining's rmse: 1.02229\tvalid_1's rmse: 1.31903\n",
      "Model training done in 19.1995370388031 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3190325035907227\n",
      "Time passed: 955.2904765605927 seconds.\n",
      "Average big fold 7 error: 1.271766309494987\n",
      "Total big fold 7 std 0.03232\n",
      "Fitting big fold 8 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.30305\tvalid_1's rmse: 1.40254\n",
      "[400]\ttraining's rmse: 1.14375\tvalid_1's rmse: 1.31761\n",
      "[600]\ttraining's rmse: 1.06796\tvalid_1's rmse: 1.29783\n",
      "[800]\ttraining's rmse: 1.01639\tvalid_1's rmse: 1.29372\n",
      "[1000]\ttraining's rmse: 0.972358\tvalid_1's rmse: 1.2927\n",
      "[1200]\ttraining's rmse: 0.932266\tvalid_1's rmse: 1.29286\n",
      "Early stopping, best iteration is:\n",
      "[1146]\ttraining's rmse: 0.943098\tvalid_1's rmse: 1.2924\n",
      "Model training done in 20.80696749687195 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2923969144306706\n",
      "Time passed: 978.3730273246765 seconds.\n",
      "Fitting big fold 8 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.329\tvalid_1's rmse: 1.39082\n",
      "[400]\ttraining's rmse: 1.18324\tvalid_1's rmse: 1.30291\n",
      "[600]\ttraining's rmse: 1.11816\tvalid_1's rmse: 1.2846\n",
      "[800]\ttraining's rmse: 1.07469\tvalid_1's rmse: 1.28264\n",
      "[1000]\ttraining's rmse: 1.03768\tvalid_1's rmse: 1.28283\n",
      "Early stopping, best iteration is:\n",
      "[919]\ttraining's rmse: 1.05299\tvalid_1's rmse: 1.28095\n",
      "Model training done in 20.64675807952881 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2809538156207223\n",
      "Time passed: 1001.0063922405243 seconds.\n",
      "Fitting big fold 8 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.333\tvalid_1's rmse: 1.34137\n",
      "[400]\ttraining's rmse: 1.1886\tvalid_1's rmse: 1.25908\n",
      "[600]\ttraining's rmse: 1.12321\tvalid_1's rmse: 1.24356\n",
      "[800]\ttraining's rmse: 1.0802\tvalid_1's rmse: 1.24128\n",
      "[1000]\ttraining's rmse: 1.04352\tvalid_1's rmse: 1.24078\n",
      "Early stopping, best iteration is:\n",
      "[837]\ttraining's rmse: 1.07314\tvalid_1's rmse: 1.24033\n",
      "Model training done in 17.9464750289917 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2403302087608472\n",
      "Time passed: 1020.8361990451813 seconds.\n",
      "Fitting big fold 8 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3252\tvalid_1's rmse: 1.40048\n",
      "[400]\ttraining's rmse: 1.18092\tvalid_1's rmse: 1.31196\n",
      "[600]\ttraining's rmse: 1.11447\tvalid_1's rmse: 1.28973\n",
      "[800]\ttraining's rmse: 1.07078\tvalid_1's rmse: 1.28229\n",
      "[1000]\ttraining's rmse: 1.03285\tvalid_1's rmse: 1.27979\n",
      "[1200]\ttraining's rmse: 0.997598\tvalid_1's rmse: 1.27771\n",
      "[1400]\ttraining's rmse: 0.964747\tvalid_1's rmse: 1.27662\n",
      "[1600]\ttraining's rmse: 0.933594\tvalid_1's rmse: 1.27552\n",
      "[1800]\ttraining's rmse: 0.903749\tvalid_1's rmse: 1.27413\n",
      "[2000]\ttraining's rmse: 0.875668\tvalid_1's rmse: 1.27359\n",
      "[2200]\ttraining's rmse: 0.849714\tvalid_1's rmse: 1.27299\n",
      "[2400]\ttraining's rmse: 0.824762\tvalid_1's rmse: 1.2727\n",
      "Early stopping, best iteration is:\n",
      "[2313]\ttraining's rmse: 0.835392\tvalid_1's rmse: 1.27252\n",
      "Model training done in 40.40252995491028 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2725161012138397\n",
      "Time passed: 1064.5047895908356 seconds.\n",
      "Fitting big fold 8 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32606\tvalid_1's rmse: 1.38224\n",
      "[400]\ttraining's rmse: 1.18042\tvalid_1's rmse: 1.29018\n",
      "[600]\ttraining's rmse: 1.11415\tvalid_1's rmse: 1.2688\n",
      "[800]\ttraining's rmse: 1.06901\tvalid_1's rmse: 1.26257\n",
      "[1000]\ttraining's rmse: 1.03233\tvalid_1's rmse: 1.26087\n",
      "[1200]\ttraining's rmse: 0.996815\tvalid_1's rmse: 1.25799\n",
      "[1400]\ttraining's rmse: 0.962612\tvalid_1's rmse: 1.25693\n",
      "[1600]\ttraining's rmse: 0.930576\tvalid_1's rmse: 1.25638\n",
      "[1800]\ttraining's rmse: 0.901055\tvalid_1's rmse: 1.25584\n",
      "[2000]\ttraining's rmse: 0.87279\tvalid_1's rmse: 1.25439\n",
      "[2200]\ttraining's rmse: 0.845734\tvalid_1's rmse: 1.25391\n",
      "[2400]\ttraining's rmse: 0.821533\tvalid_1's rmse: 1.25293\n",
      "[2600]\ttraining's rmse: 0.797358\tvalid_1's rmse: 1.25272\n",
      "[2800]\ttraining's rmse: 0.776439\tvalid_1's rmse: 1.25271\n",
      "Early stopping, best iteration is:\n",
      "[2678]\ttraining's rmse: 0.788938\tvalid_1's rmse: 1.25244\n",
      "Model training done in 43.104328870773315 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2524434233336583\n",
      "Time passed: 1111.240802526474 seconds.\n",
      "Average big fold 8 error: 1.2735387992639697\n",
      "Total big fold 8 std 0.01892\n",
      "Fitting big fold 9 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.30949\tvalid_1's rmse: 1.39626\n",
      "[400]\ttraining's rmse: 1.15282\tvalid_1's rmse: 1.30787\n",
      "[600]\ttraining's rmse: 1.07809\tvalid_1's rmse: 1.28695\n",
      "[800]\ttraining's rmse: 1.02662\tvalid_1's rmse: 1.28096\n",
      "[1000]\ttraining's rmse: 0.982878\tvalid_1's rmse: 1.27802\n",
      "[1200]\ttraining's rmse: 0.94355\tvalid_1's rmse: 1.27648\n",
      "[1400]\ttraining's rmse: 0.905554\tvalid_1's rmse: 1.27661\n",
      "Early stopping, best iteration is:\n",
      "[1264]\ttraining's rmse: 0.931021\tvalid_1's rmse: 1.27621\n",
      "Model training done in 21.653565883636475 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2762074049746641\n",
      "Time passed: 1135.1434123516083 seconds.\n",
      "Fitting big fold 9 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32845\tvalid_1's rmse: 1.38131\n",
      "[400]\ttraining's rmse: 1.1842\tvalid_1's rmse: 1.29874\n",
      "[600]\ttraining's rmse: 1.11892\tvalid_1's rmse: 1.28207\n",
      "[800]\ttraining's rmse: 1.07489\tvalid_1's rmse: 1.27827\n",
      "[1000]\ttraining's rmse: 1.03852\tvalid_1's rmse: 1.27864\n",
      "[1200]\ttraining's rmse: 1.00539\tvalid_1's rmse: 1.27796\n",
      "Early stopping, best iteration is:\n",
      "[1193]\ttraining's rmse: 1.00649\tvalid_1's rmse: 1.2776\n",
      "Model training done in 24.34313726425171 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2776006128555262\n",
      "Time passed: 1161.801305770874 seconds.\n",
      "Fitting big fold 9 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32713\tvalid_1's rmse: 1.38038\n",
      "[400]\ttraining's rmse: 1.18207\tvalid_1's rmse: 1.29357\n",
      "[600]\ttraining's rmse: 1.11693\tvalid_1's rmse: 1.27401\n",
      "[800]\ttraining's rmse: 1.07337\tvalid_1's rmse: 1.26782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's rmse: 1.0348\tvalid_1's rmse: 1.26581\n",
      "[1200]\ttraining's rmse: 0.999365\tvalid_1's rmse: 1.26396\n",
      "[1400]\ttraining's rmse: 0.967452\tvalid_1's rmse: 1.26394\n",
      "[1600]\ttraining's rmse: 0.937955\tvalid_1's rmse: 1.26405\n",
      "[1800]\ttraining's rmse: 0.909265\tvalid_1's rmse: 1.26258\n",
      "[2000]\ttraining's rmse: 0.880964\tvalid_1's rmse: 1.26181\n",
      "[2200]\ttraining's rmse: 0.854727\tvalid_1's rmse: 1.26109\n",
      "Early stopping, best iteration is:\n",
      "[2176]\ttraining's rmse: 0.857597\tvalid_1's rmse: 1.26056\n",
      "Model training done in 40.25103259086609 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2605606915746326\n",
      "Time passed: 1205.3064641952515 seconds.\n",
      "Fitting big fold 9 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32078\tvalid_1's rmse: 1.41126\n",
      "[400]\ttraining's rmse: 1.17676\tvalid_1's rmse: 1.32137\n",
      "[600]\ttraining's rmse: 1.11151\tvalid_1's rmse: 1.30282\n",
      "[800]\ttraining's rmse: 1.06772\tvalid_1's rmse: 1.29972\n",
      "[1000]\ttraining's rmse: 1.03062\tvalid_1's rmse: 1.29776\n",
      "[1200]\ttraining's rmse: 0.994794\tvalid_1's rmse: 1.29772\n",
      "Early stopping, best iteration is:\n",
      "[1056]\ttraining's rmse: 1.02033\tvalid_1's rmse: 1.29725\n",
      "Model training done in 21.776559829711914 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2972525677830415\n",
      "Time passed: 1229.1534402370453 seconds.\n",
      "Fitting big fold 9 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33048\tvalid_1's rmse: 1.35457\n",
      "[400]\ttraining's rmse: 1.18646\tvalid_1's rmse: 1.26056\n",
      "[600]\ttraining's rmse: 1.119\tvalid_1's rmse: 1.23967\n",
      "[800]\ttraining's rmse: 1.07183\tvalid_1's rmse: 1.23391\n",
      "[1000]\ttraining's rmse: 1.0335\tvalid_1's rmse: 1.23167\n",
      "[1200]\ttraining's rmse: 0.996354\tvalid_1's rmse: 1.23103\n",
      "[1400]\ttraining's rmse: 0.963222\tvalid_1's rmse: 1.23029\n",
      "[1600]\ttraining's rmse: 0.930961\tvalid_1's rmse: 1.23011\n",
      "[1800]\ttraining's rmse: 0.902699\tvalid_1's rmse: 1.23056\n",
      "Early stopping, best iteration is:\n",
      "[1646]\ttraining's rmse: 0.924695\tvalid_1's rmse: 1.22977\n",
      "Model training done in 29.75788903236389 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2297692281200712\n",
      "Time passed: 1261.6344220638275 seconds.\n",
      "Average big fold 9 error: 1.2686342762862997\n",
      "Total big fold 9 std 0.0225\n",
      "Fitting big fold 10 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31466\tvalid_1's rmse: 1.39151\n",
      "[400]\ttraining's rmse: 1.15971\tvalid_1's rmse: 1.29858\n",
      "[600]\ttraining's rmse: 1.08401\tvalid_1's rmse: 1.27755\n",
      "[800]\ttraining's rmse: 1.03178\tvalid_1's rmse: 1.2731\n",
      "[1000]\ttraining's rmse: 0.988526\tvalid_1's rmse: 1.2716\n",
      "Early stopping, best iteration is:\n",
      "[980]\ttraining's rmse: 0.992649\tvalid_1's rmse: 1.27141\n",
      "Model training done in 19.24523115158081 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.271414938826445\n",
      "Time passed: 1282.9376962184906 seconds.\n",
      "Fitting big fold 10 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33709\tvalid_1's rmse: 1.29529\n",
      "[400]\ttraining's rmse: 1.19492\tvalid_1's rmse: 1.20513\n",
      "[600]\ttraining's rmse: 1.13108\tvalid_1's rmse: 1.18699\n",
      "[800]\ttraining's rmse: 1.0871\tvalid_1's rmse: 1.1838\n",
      "[1000]\ttraining's rmse: 1.04966\tvalid_1's rmse: 1.18181\n",
      "Early stopping, best iteration is:\n",
      "[997]\ttraining's rmse: 1.05015\tvalid_1's rmse: 1.18174\n",
      "Model training done in 22.62963104248047 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.181743287649337\n",
      "Time passed: 1307.6188271045685 seconds.\n",
      "Fitting big fold 10 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32898\tvalid_1's rmse: 1.36664\n",
      "[400]\ttraining's rmse: 1.18237\tvalid_1's rmse: 1.28693\n",
      "[600]\ttraining's rmse: 1.11613\tvalid_1's rmse: 1.27102\n",
      "[800]\ttraining's rmse: 1.0717\tvalid_1's rmse: 1.27008\n",
      "Early stopping, best iteration is:\n",
      "[742]\ttraining's rmse: 1.08412\tvalid_1's rmse: 1.26918\n",
      "Model training done in 19.31025266647339 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2691821454672687\n",
      "Time passed: 1328.7705824375153 seconds.\n",
      "Fitting big fold 10 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32276\tvalid_1's rmse: 1.41071\n",
      "[400]\ttraining's rmse: 1.17533\tvalid_1's rmse: 1.33065\n",
      "[600]\ttraining's rmse: 1.10917\tvalid_1's rmse: 1.31186\n",
      "[800]\ttraining's rmse: 1.06455\tvalid_1's rmse: 1.30717\n",
      "[1000]\ttraining's rmse: 1.02607\tvalid_1's rmse: 1.30559\n",
      "[1200]\ttraining's rmse: 0.992556\tvalid_1's rmse: 1.30496\n",
      "[1400]\ttraining's rmse: 0.960895\tvalid_1's rmse: 1.3049\n",
      "Early stopping, best iteration is:\n",
      "[1351]\ttraining's rmse: 0.968768\tvalid_1's rmse: 1.30445\n",
      "Model training done in 26.553786516189575 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.304453711746192\n",
      "Time passed: 1357.7920203208923 seconds.\n",
      "Fitting big fold 10 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3142\tvalid_1's rmse: 1.42978\n",
      "[400]\ttraining's rmse: 1.16767\tvalid_1's rmse: 1.34674\n",
      "[600]\ttraining's rmse: 1.10141\tvalid_1's rmse: 1.32856\n",
      "[800]\ttraining's rmse: 1.05603\tvalid_1's rmse: 1.32339\n",
      "[1000]\ttraining's rmse: 1.01825\tvalid_1's rmse: 1.32245\n",
      "[1200]\ttraining's rmse: 0.982973\tvalid_1's rmse: 1.32181\n",
      "Early stopping, best iteration is:\n",
      "[1098]\ttraining's rmse: 1.00037\tvalid_1's rmse: 1.32161\n",
      "Model training done in 23.582210540771484 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3216149698929083\n",
      "Time passed: 1383.469777584076 seconds.\n",
      "Average big fold 10 error: 1.278056590335265\n",
      "Total big fold 10 std 0.04825\n",
      "Fitting big fold 11 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.30685\tvalid_1's rmse: 1.40482\n",
      "[400]\ttraining's rmse: 1.15241\tvalid_1's rmse: 1.31405\n",
      "[600]\ttraining's rmse: 1.07697\tvalid_1's rmse: 1.29062\n",
      "[800]\ttraining's rmse: 1.02615\tvalid_1's rmse: 1.28444\n",
      "[1000]\ttraining's rmse: 0.981819\tvalid_1's rmse: 1.28249\n",
      "[1200]\ttraining's rmse: 0.943532\tvalid_1's rmse: 1.28198\n",
      "[1400]\ttraining's rmse: 0.907646\tvalid_1's rmse: 1.28215\n",
      "Early stopping, best iteration is:\n",
      "[1304]\ttraining's rmse: 0.925146\tvalid_1's rmse: 1.28174\n",
      "Model training done in 24.87802505493164 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.281742216099303\n",
      "Time passed: 1410.8125910758972 seconds.\n",
      "Fitting big fold 11 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3319\tvalid_1's rmse: 1.32845\n",
      "[400]\ttraining's rmse: 1.18643\tvalid_1's rmse: 1.26105\n",
      "[600]\ttraining's rmse: 1.12151\tvalid_1's rmse: 1.24999\n",
      "[800]\ttraining's rmse: 1.07728\tvalid_1's rmse: 1.24683\n",
      "[1000]\ttraining's rmse: 1.04068\tvalid_1's rmse: 1.24689\n",
      "Early stopping, best iteration is:\n",
      "[837]\ttraining's rmse: 1.07011\tvalid_1's rmse: 1.24583\n",
      "Model training done in 19.873674154281616 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2458324791984596\n",
      "Time passed: 1432.5796201229095 seconds.\n",
      "Fitting big fold 11 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33162\tvalid_1's rmse: 1.35521\n",
      "[400]\ttraining's rmse: 1.19005\tvalid_1's rmse: 1.25109\n",
      "[600]\ttraining's rmse: 1.12521\tvalid_1's rmse: 1.22461\n",
      "[800]\ttraining's rmse: 1.08112\tvalid_1's rmse: 1.21643\n",
      "[1000]\ttraining's rmse: 1.04385\tvalid_1's rmse: 1.2128\n",
      "[1200]\ttraining's rmse: 1.00797\tvalid_1's rmse: 1.21077\n",
      "[1400]\ttraining's rmse: 0.974841\tvalid_1's rmse: 1.20863\n",
      "[1600]\ttraining's rmse: 0.94501\tvalid_1's rmse: 1.20779\n",
      "[1800]\ttraining's rmse: 0.916447\tvalid_1's rmse: 1.20659\n",
      "[2000]\ttraining's rmse: 0.888006\tvalid_1's rmse: 1.20535\n",
      "[2200]\ttraining's rmse: 0.860548\tvalid_1's rmse: 1.20497\n",
      "[2400]\ttraining's rmse: 0.83633\tvalid_1's rmse: 1.20463\n",
      "[2600]\ttraining's rmse: 0.811674\tvalid_1's rmse: 1.20426\n",
      "[2800]\ttraining's rmse: 0.789035\tvalid_1's rmse: 1.20427\n",
      "Early stopping, best iteration is:\n",
      "[2658]\ttraining's rmse: 0.804617\tvalid_1's rmse: 1.20383\n",
      "Model training done in 49.609482288360596 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2038337439278035\n",
      "Time passed: 1486.12411236763 seconds.\n",
      "Fitting big fold 11 out of 20 and sub fold 4 out of 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32767\tvalid_1's rmse: 1.38042\n",
      "[400]\ttraining's rmse: 1.1824\tvalid_1's rmse: 1.2928\n",
      "[600]\ttraining's rmse: 1.11579\tvalid_1's rmse: 1.27341\n",
      "[800]\ttraining's rmse: 1.07066\tvalid_1's rmse: 1.2675\n",
      "[1000]\ttraining's rmse: 1.03348\tvalid_1's rmse: 1.26705\n",
      "[1200]\ttraining's rmse: 0.997278\tvalid_1's rmse: 1.26393\n",
      "[1400]\ttraining's rmse: 0.965053\tvalid_1's rmse: 1.26296\n",
      "[1600]\ttraining's rmse: 0.934169\tvalid_1's rmse: 1.26233\n",
      "[1800]\ttraining's rmse: 0.904828\tvalid_1's rmse: 1.2616\n",
      "[2000]\ttraining's rmse: 0.877706\tvalid_1's rmse: 1.26127\n",
      "[2200]\ttraining's rmse: 0.852665\tvalid_1's rmse: 1.26121\n",
      "Early stopping, best iteration is:\n",
      "[2114]\ttraining's rmse: 0.863459\tvalid_1's rmse: 1.2609\n",
      "Model training done in 40.226168155670166 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2609022760413195\n",
      "Time passed: 1529.5628237724304 seconds.\n",
      "Fitting big fold 11 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31725\tvalid_1's rmse: 1.40997\n",
      "[400]\ttraining's rmse: 1.16889\tvalid_1's rmse: 1.3246\n",
      "[600]\ttraining's rmse: 1.1013\tvalid_1's rmse: 1.30558\n",
      "[800]\ttraining's rmse: 1.05671\tvalid_1's rmse: 1.30111\n",
      "[1000]\ttraining's rmse: 1.01956\tvalid_1's rmse: 1.2996\n",
      "[1200]\ttraining's rmse: 0.984182\tvalid_1's rmse: 1.29868\n",
      "[1400]\ttraining's rmse: 0.950977\tvalid_1's rmse: 1.29841\n",
      "Early stopping, best iteration is:\n",
      "[1379]\ttraining's rmse: 0.95466\tvalid_1's rmse: 1.29814\n",
      "Model training done in 28.175517082214355 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.298141299591084\n",
      "Time passed: 1560.2171366214752 seconds.\n",
      "Average big fold 11 error: 1.2680404029394112\n",
      "Total big fold 11 std 0.03245\n",
      "Fitting big fold 12 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32887\tvalid_1's rmse: 1.37984\n",
      "[400]\ttraining's rmse: 1.17234\tvalid_1's rmse: 1.28371\n",
      "[600]\ttraining's rmse: 1.09648\tvalid_1's rmse: 1.26019\n",
      "[800]\ttraining's rmse: 1.0423\tvalid_1's rmse: 1.25266\n",
      "[1000]\ttraining's rmse: 0.995433\tvalid_1's rmse: 1.25211\n",
      "Early stopping, best iteration is:\n",
      "[916]\ttraining's rmse: 1.01492\tvalid_1's rmse: 1.25144\n",
      "Model training done in 18.791679859161377 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2514430378480754\n",
      "Time passed: 1581.1072628498077 seconds.\n",
      "Fitting big fold 12 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32449\tvalid_1's rmse: 1.40743\n",
      "[400]\ttraining's rmse: 1.18202\tvalid_1's rmse: 1.32493\n",
      "[600]\ttraining's rmse: 1.1175\tvalid_1's rmse: 1.30691\n",
      "[800]\ttraining's rmse: 1.07409\tvalid_1's rmse: 1.30272\n",
      "[1000]\ttraining's rmse: 1.03757\tvalid_1's rmse: 1.30106\n",
      "[1200]\ttraining's rmse: 1.00474\tvalid_1's rmse: 1.30107\n",
      "Early stopping, best iteration is:\n",
      "[1157]\ttraining's rmse: 1.01189\tvalid_1's rmse: 1.30026\n",
      "Model training done in 25.169910430908203 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3002562157152202\n",
      "Time passed: 1608.4548614025116 seconds.\n",
      "Fitting big fold 12 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32802\tvalid_1's rmse: 1.37573\n",
      "[400]\ttraining's rmse: 1.18371\tvalid_1's rmse: 1.29129\n",
      "[600]\ttraining's rmse: 1.11852\tvalid_1's rmse: 1.26966\n",
      "[800]\ttraining's rmse: 1.07544\tvalid_1's rmse: 1.26382\n",
      "[1000]\ttraining's rmse: 1.03805\tvalid_1's rmse: 1.25991\n",
      "[1200]\ttraining's rmse: 1.0046\tvalid_1's rmse: 1.25774\n",
      "[1400]\ttraining's rmse: 0.973332\tvalid_1's rmse: 1.25548\n",
      "[1600]\ttraining's rmse: 0.943925\tvalid_1's rmse: 1.25298\n",
      "[1800]\ttraining's rmse: 0.915793\tvalid_1's rmse: 1.25168\n",
      "Early stopping, best iteration is:\n",
      "[1767]\ttraining's rmse: 0.920624\tvalid_1's rmse: 1.25161\n",
      "Model training done in 35.849738121032715 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2516143591949922\n",
      "Time passed: 1647.2102000713348 seconds.\n",
      "Fitting big fold 12 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32751\tvalid_1's rmse: 1.38216\n",
      "[400]\ttraining's rmse: 1.18241\tvalid_1's rmse: 1.29676\n",
      "[600]\ttraining's rmse: 1.11517\tvalid_1's rmse: 1.27943\n",
      "[800]\ttraining's rmse: 1.07039\tvalid_1's rmse: 1.27312\n",
      "[1000]\ttraining's rmse: 1.03208\tvalid_1's rmse: 1.27164\n",
      "Early stopping, best iteration is:\n",
      "[977]\ttraining's rmse: 1.03631\tvalid_1's rmse: 1.27124\n",
      "Model training done in 21.806976079940796 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2712365195911672\n",
      "Time passed: 1671.2193350791931 seconds.\n",
      "Fitting big fold 12 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31401\tvalid_1's rmse: 1.41768\n",
      "[400]\ttraining's rmse: 1.16635\tvalid_1's rmse: 1.3366\n",
      "[600]\ttraining's rmse: 1.09863\tvalid_1's rmse: 1.31969\n",
      "[800]\ttraining's rmse: 1.05415\tvalid_1's rmse: 1.31665\n",
      "[1000]\ttraining's rmse: 1.01609\tvalid_1's rmse: 1.31566\n",
      "[1200]\ttraining's rmse: 0.981029\tvalid_1's rmse: 1.31563\n",
      "Early stopping, best iteration is:\n",
      "[1009]\ttraining's rmse: 1.01453\tvalid_1's rmse: 1.31551\n",
      "Model training done in 21.555160760879517 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3155062734051597\n",
      "Time passed: 1694.9436452388763 seconds.\n",
      "Average big fold 12 error: 1.2727132806035057\n",
      "Total big fold 12 std 0.02588\n",
      "Fitting big fold 13 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31164\tvalid_1's rmse: 1.39358\n",
      "[400]\ttraining's rmse: 1.15397\tvalid_1's rmse: 1.31305\n",
      "[600]\ttraining's rmse: 1.07806\tvalid_1's rmse: 1.29603\n",
      "[800]\ttraining's rmse: 1.02597\tvalid_1's rmse: 1.29177\n",
      "[1000]\ttraining's rmse: 0.98045\tvalid_1's rmse: 1.29148\n",
      "[1200]\ttraining's rmse: 0.93694\tvalid_1's rmse: 1.29183\n",
      "Early stopping, best iteration is:\n",
      "[1074]\ttraining's rmse: 0.96346\tvalid_1's rmse: 1.29077\n",
      "Model training done in 21.151877403259277 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2907668662893288\n",
      "Time passed: 1718.3180708885193 seconds.\n",
      "Fitting big fold 13 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32869\tvalid_1's rmse: 1.36346\n",
      "[400]\ttraining's rmse: 1.18472\tvalid_1's rmse: 1.27727\n",
      "[600]\ttraining's rmse: 1.11967\tvalid_1's rmse: 1.26178\n",
      "[800]\ttraining's rmse: 1.07681\tvalid_1's rmse: 1.25828\n",
      "Early stopping, best iteration is:\n",
      "[763]\ttraining's rmse: 1.08431\tvalid_1's rmse: 1.25779\n",
      "Model training done in 18.31266498565674 seconds.\n",
      "17.50439003707821 17.43028206677064\n",
      "Fold error 1.2577904046163126\n",
      "Time passed: 1738.459882736206 seconds.\n",
      "Fitting big fold 13 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32871\tvalid_1's rmse: 1.3768\n",
      "[400]\ttraining's rmse: 1.18298\tvalid_1's rmse: 1.29147\n",
      "[600]\ttraining's rmse: 1.11819\tvalid_1's rmse: 1.27229\n",
      "[800]\ttraining's rmse: 1.07533\tvalid_1's rmse: 1.26624\n",
      "[1000]\ttraining's rmse: 1.03737\tvalid_1's rmse: 1.26528\n",
      "Early stopping, best iteration is:\n",
      "[973]\ttraining's rmse: 1.04224\tvalid_1's rmse: 1.26488\n",
      "Model training done in 21.521337032318115 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.264876476430778\n",
      "Time passed: 1762.0127928256989 seconds.\n",
      "Fitting big fold 13 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32226\tvalid_1's rmse: 1.40726\n",
      "[400]\ttraining's rmse: 1.17777\tvalid_1's rmse: 1.31256\n",
      "[600]\ttraining's rmse: 1.11074\tvalid_1's rmse: 1.29001\n",
      "[800]\ttraining's rmse: 1.0668\tvalid_1's rmse: 1.2849\n",
      "[1000]\ttraining's rmse: 1.0293\tvalid_1's rmse: 1.28265\n",
      "[1200]\ttraining's rmse: 0.994114\tvalid_1's rmse: 1.28127\n",
      "[1400]\ttraining's rmse: 0.960157\tvalid_1's rmse: 1.28069\n",
      "Early stopping, best iteration is:\n",
      "[1271]\ttraining's rmse: 0.981814\tvalid_1's rmse: 1.2806\n",
      "Model training done in 25.57418441772461 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2805974700060707\n",
      "Time passed: 1789.9174795150757 seconds.\n",
      "Fitting big fold 13 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32753\tvalid_1's rmse: 1.38181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttraining's rmse: 1.18158\tvalid_1's rmse: 1.28465\n",
      "[600]\ttraining's rmse: 1.11454\tvalid_1's rmse: 1.25871\n",
      "[800]\ttraining's rmse: 1.07059\tvalid_1's rmse: 1.25088\n",
      "[1000]\ttraining's rmse: 1.03341\tvalid_1's rmse: 1.24693\n",
      "[1200]\ttraining's rmse: 0.999206\tvalid_1's rmse: 1.24445\n",
      "[1400]\ttraining's rmse: 0.966014\tvalid_1's rmse: 1.24273\n",
      "[1600]\ttraining's rmse: 0.935116\tvalid_1's rmse: 1.24256\n",
      "[1800]\ttraining's rmse: 0.906053\tvalid_1's rmse: 1.24213\n",
      "[2000]\ttraining's rmse: 0.879301\tvalid_1's rmse: 1.24192\n",
      "Early stopping, best iteration is:\n",
      "[1860]\ttraining's rmse: 0.897758\tvalid_1's rmse: 1.2416\n",
      "Model training done in 34.495545864105225 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2415978678757524\n",
      "Time passed: 1827.2854216098785 seconds.\n",
      "Average big fold 13 error: 1.2728356414564723\n",
      "Total big fold 13 std 0.01723\n",
      "Fitting big fold 14 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31432\tvalid_1's rmse: 1.37729\n",
      "[400]\ttraining's rmse: 1.15668\tvalid_1's rmse: 1.29129\n",
      "[600]\ttraining's rmse: 1.08322\tvalid_1's rmse: 1.27444\n",
      "[800]\ttraining's rmse: 1.03212\tvalid_1's rmse: 1.27101\n",
      "[1000]\ttraining's rmse: 0.989397\tvalid_1's rmse: 1.27108\n",
      "[1200]\ttraining's rmse: 0.950487\tvalid_1's rmse: 1.27085\n",
      "[1400]\ttraining's rmse: 0.913795\tvalid_1's rmse: 1.27094\n",
      "Early stopping, best iteration is:\n",
      "[1248]\ttraining's rmse: 0.941898\tvalid_1's rmse: 1.27041\n",
      "Model training done in 22.30945634841919 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2704084511019542\n",
      "Time passed: 1851.911880016327 seconds.\n",
      "Fitting big fold 14 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32714\tvalid_1's rmse: 1.3861\n",
      "[400]\ttraining's rmse: 1.18182\tvalid_1's rmse: 1.30871\n",
      "[600]\ttraining's rmse: 1.11657\tvalid_1's rmse: 1.29292\n",
      "[800]\ttraining's rmse: 1.07325\tvalid_1's rmse: 1.29135\n",
      "[1000]\ttraining's rmse: 1.03634\tvalid_1's rmse: 1.29152\n",
      "Early stopping, best iteration is:\n",
      "[835]\ttraining's rmse: 1.06657\tvalid_1's rmse: 1.29095\n",
      "Model training done in 19.03937530517578 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2909494356824907\n",
      "Time passed: 1872.9151964187622 seconds.\n",
      "Fitting big fold 14 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32334\tvalid_1's rmse: 1.40582\n",
      "[400]\ttraining's rmse: 1.17923\tvalid_1's rmse: 1.3192\n",
      "[600]\ttraining's rmse: 1.11237\tvalid_1's rmse: 1.29823\n",
      "[800]\ttraining's rmse: 1.06773\tvalid_1's rmse: 1.29291\n",
      "[1000]\ttraining's rmse: 1.03092\tvalid_1's rmse: 1.2911\n",
      "[1200]\ttraining's rmse: 0.996377\tvalid_1's rmse: 1.29213\n",
      "Early stopping, best iteration is:\n",
      "[1032]\ttraining's rmse: 1.02531\tvalid_1's rmse: 1.29062\n",
      "Model training done in 23.2832670211792 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2906174195465872\n",
      "Time passed: 1898.2395544052124 seconds.\n",
      "Fitting big fold 14 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32627\tvalid_1's rmse: 1.3946\n",
      "[400]\ttraining's rmse: 1.1813\tvalid_1's rmse: 1.30263\n",
      "[600]\ttraining's rmse: 1.116\tvalid_1's rmse: 1.27865\n",
      "[800]\ttraining's rmse: 1.07022\tvalid_1's rmse: 1.27265\n",
      "[1000]\ttraining's rmse: 1.03239\tvalid_1's rmse: 1.27028\n",
      "[1200]\ttraining's rmse: 0.996352\tvalid_1's rmse: 1.26963\n",
      "Early stopping, best iteration is:\n",
      "[1080]\ttraining's rmse: 1.01774\tvalid_1's rmse: 1.26899\n",
      "Model training done in 23.698978900909424 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2689899688093433\n",
      "Time passed: 1924.1406638622284 seconds.\n",
      "Fitting big fold 14 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3245\tvalid_1's rmse: 1.38719\n",
      "[400]\ttraining's rmse: 1.17767\tvalid_1's rmse: 1.29748\n",
      "[600]\ttraining's rmse: 1.11133\tvalid_1's rmse: 1.27609\n",
      "[800]\ttraining's rmse: 1.0665\tvalid_1's rmse: 1.26951\n",
      "[1000]\ttraining's rmse: 1.02742\tvalid_1's rmse: 1.26674\n",
      "[1200]\ttraining's rmse: 0.991138\tvalid_1's rmse: 1.26567\n",
      "[1400]\ttraining's rmse: 0.956727\tvalid_1's rmse: 1.26502\n",
      "Early stopping, best iteration is:\n",
      "[1330]\ttraining's rmse: 0.968611\tvalid_1's rmse: 1.26439\n",
      "Model training done in 26.3111252784729 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2643887203531805\n",
      "Time passed: 1952.851392507553 seconds.\n",
      "Average big fold 14 error: 1.2736796203908158\n",
      "Total big fold 14 std 0.01137\n",
      "Fitting big fold 15 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.30737\tvalid_1's rmse: 1.40019\n",
      "[400]\ttraining's rmse: 1.15117\tvalid_1's rmse: 1.30792\n",
      "[600]\ttraining's rmse: 1.07664\tvalid_1's rmse: 1.28529\n",
      "[800]\ttraining's rmse: 1.02595\tvalid_1's rmse: 1.27978\n",
      "[1000]\ttraining's rmse: 0.983014\tvalid_1's rmse: 1.27788\n",
      "[1200]\ttraining's rmse: 0.944012\tvalid_1's rmse: 1.27834\n",
      "Early stopping, best iteration is:\n",
      "[1008]\ttraining's rmse: 0.981085\tvalid_1's rmse: 1.27775\n",
      "Model training done in 19.0292706489563 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2777527994452627\n",
      "Time passed: 1973.9457833766937 seconds.\n",
      "Fitting big fold 15 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32749\tvalid_1's rmse: 1.39946\n",
      "[400]\ttraining's rmse: 1.18417\tvalid_1's rmse: 1.32156\n",
      "[600]\ttraining's rmse: 1.11918\tvalid_1's rmse: 1.30714\n",
      "[800]\ttraining's rmse: 1.07687\tvalid_1's rmse: 1.30215\n",
      "[1000]\ttraining's rmse: 1.04064\tvalid_1's rmse: 1.30076\n",
      "[1200]\ttraining's rmse: 1.00674\tvalid_1's rmse: 1.29933\n",
      "[1400]\ttraining's rmse: 0.974483\tvalid_1's rmse: 1.29843\n",
      "[1600]\ttraining's rmse: 0.943715\tvalid_1's rmse: 1.29827\n",
      "Early stopping, best iteration is:\n",
      "[1472]\ttraining's rmse: 0.96335\tvalid_1's rmse: 1.2976\n",
      "Model training done in 29.236162424087524 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2975993351407926\n",
      "Time passed: 2005.6515114307404 seconds.\n",
      "Fitting big fold 15 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32744\tvalid_1's rmse: 1.39254\n",
      "[400]\ttraining's rmse: 1.18559\tvalid_1's rmse: 1.29958\n",
      "[600]\ttraining's rmse: 1.12019\tvalid_1's rmse: 1.27747\n",
      "[800]\ttraining's rmse: 1.07714\tvalid_1's rmse: 1.27053\n",
      "[1000]\ttraining's rmse: 1.0395\tvalid_1's rmse: 1.26979\n",
      "[1200]\ttraining's rmse: 1.00544\tvalid_1's rmse: 1.26615\n",
      "[1400]\ttraining's rmse: 0.972881\tvalid_1's rmse: 1.26632\n",
      "Early stopping, best iteration is:\n",
      "[1200]\ttraining's rmse: 1.00544\tvalid_1's rmse: 1.26615\n",
      "Model training done in 24.41760015487671 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2661475118101055\n",
      "Time passed: 2032.2776687145233 seconds.\n",
      "Fitting big fold 15 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33208\tvalid_1's rmse: 1.34557\n",
      "[400]\ttraining's rmse: 1.18653\tvalid_1's rmse: 1.25868\n",
      "[600]\ttraining's rmse: 1.11921\tvalid_1's rmse: 1.23969\n",
      "[800]\ttraining's rmse: 1.07544\tvalid_1's rmse: 1.23499\n",
      "[1000]\ttraining's rmse: 1.03738\tvalid_1's rmse: 1.23482\n",
      "[1200]\ttraining's rmse: 1.00283\tvalid_1's rmse: 1.23492\n",
      "Early stopping, best iteration is:\n",
      "[1118]\ttraining's rmse: 1.01681\tvalid_1's rmse: 1.23423\n",
      "Model training done in 23.71315312385559 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2342330492766762\n",
      "Time passed: 2058.172330379486 seconds.\n",
      "Fitting big fold 15 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32193\tvalid_1's rmse: 1.39166\n",
      "[400]\ttraining's rmse: 1.17332\tvalid_1's rmse: 1.30758\n",
      "[600]\ttraining's rmse: 1.10443\tvalid_1's rmse: 1.28949\n",
      "[800]\ttraining's rmse: 1.05882\tvalid_1's rmse: 1.28355\n",
      "[1000]\ttraining's rmse: 1.01908\tvalid_1's rmse: 1.28254\n",
      "[1200]\ttraining's rmse: 0.982793\tvalid_1's rmse: 1.2825\n",
      "[1400]\ttraining's rmse: 0.948545\tvalid_1's rmse: 1.28139\n",
      "[1600]\ttraining's rmse: 0.916027\tvalid_1's rmse: 1.28148\n",
      "Early stopping, best iteration is:\n",
      "[1530]\ttraining's rmse: 0.927144\tvalid_1's rmse: 1.28076\n",
      "Model training done in 29.053221702575684 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2807617063614625\n",
      "Time passed: 2089.8101596832275 seconds.\n",
      "Average big fold 15 error: 1.2719584793191103\n",
      "Total big fold 15 std 0.02109\n",
      "Fitting big fold 16 out of 20 and sub fold 1 out of 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.30692\tvalid_1's rmse: 1.38425\n",
      "[400]\ttraining's rmse: 1.14821\tvalid_1's rmse: 1.30254\n",
      "[600]\ttraining's rmse: 1.07489\tvalid_1's rmse: 1.28594\n",
      "[800]\ttraining's rmse: 1.02547\tvalid_1's rmse: 1.28259\n",
      "[1000]\ttraining's rmse: 0.982502\tvalid_1's rmse: 1.28224\n",
      "Early stopping, best iteration is:\n",
      "[975]\ttraining's rmse: 0.987969\tvalid_1's rmse: 1.28203\n",
      "Model training done in 18.615153312683105 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2820324368498253\n",
      "Time passed: 2110.464861869812 seconds.\n",
      "Fitting big fold 16 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32922\tvalid_1's rmse: 1.39475\n",
      "[400]\ttraining's rmse: 1.18781\tvalid_1's rmse: 1.28792\n",
      "[600]\ttraining's rmse: 1.12327\tvalid_1's rmse: 1.2614\n",
      "[800]\ttraining's rmse: 1.07992\tvalid_1's rmse: 1.25617\n",
      "[1000]\ttraining's rmse: 1.04282\tvalid_1's rmse: 1.25285\n",
      "[1200]\ttraining's rmse: 1.00889\tvalid_1's rmse: 1.25181\n",
      "[1400]\ttraining's rmse: 0.975869\tvalid_1's rmse: 1.25342\n",
      "Early stopping, best iteration is:\n",
      "[1224]\ttraining's rmse: 1.0048\tvalid_1's rmse: 1.25121\n",
      "Model training done in 25.12929677963257 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.251205172488208\n",
      "Time passed: 2137.843712568283 seconds.\n",
      "Fitting big fold 16 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33162\tvalid_1's rmse: 1.35484\n",
      "[400]\ttraining's rmse: 1.18904\tvalid_1's rmse: 1.25811\n",
      "[600]\ttraining's rmse: 1.12408\tvalid_1's rmse: 1.23574\n",
      "[800]\ttraining's rmse: 1.08055\tvalid_1's rmse: 1.22794\n",
      "[1000]\ttraining's rmse: 1.04377\tvalid_1's rmse: 1.2248\n",
      "[1200]\ttraining's rmse: 1.00868\tvalid_1's rmse: 1.22423\n",
      "[1400]\ttraining's rmse: 0.975509\tvalid_1's rmse: 1.22332\n",
      "[1600]\ttraining's rmse: 0.944873\tvalid_1's rmse: 1.22296\n",
      "Early stopping, best iteration is:\n",
      "[1451]\ttraining's rmse: 0.9673\tvalid_1's rmse: 1.22189\n",
      "Model training done in 29.81576108932495 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2218915496368732\n",
      "Time passed: 2170.2136833667755 seconds.\n",
      "Fitting big fold 16 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32526\tvalid_1's rmse: 1.38219\n",
      "[400]\ttraining's rmse: 1.18045\tvalid_1's rmse: 1.30134\n",
      "[600]\ttraining's rmse: 1.11386\tvalid_1's rmse: 1.28481\n",
      "[800]\ttraining's rmse: 1.06952\tvalid_1's rmse: 1.28039\n",
      "Early stopping, best iteration is:\n",
      "[778]\ttraining's rmse: 1.07402\tvalid_1's rmse: 1.28003\n",
      "Model training done in 18.323577880859375 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2800324079046153\n",
      "Time passed: 2190.290295124054 seconds.\n",
      "Fitting big fold 16 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32333\tvalid_1's rmse: 1.40658\n",
      "[400]\ttraining's rmse: 1.17512\tvalid_1's rmse: 1.31978\n",
      "[600]\ttraining's rmse: 1.10644\tvalid_1's rmse: 1.29742\n",
      "[800]\ttraining's rmse: 1.06092\tvalid_1's rmse: 1.29126\n",
      "[1000]\ttraining's rmse: 1.02393\tvalid_1's rmse: 1.28882\n",
      "[1200]\ttraining's rmse: 0.989277\tvalid_1's rmse: 1.28804\n",
      "[1400]\ttraining's rmse: 0.954972\tvalid_1's rmse: 1.28755\n",
      "Early stopping, best iteration is:\n",
      "[1346]\ttraining's rmse: 0.963586\tvalid_1's rmse: 1.28725\n",
      "Model training done in 27.538138151168823 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2872546977149366\n",
      "Time passed: 2220.237981557846 seconds.\n",
      "Average big fold 16 error: 1.2722728960804381\n",
      "Total big fold 16 std 0.02473\n",
      "Fitting big fold 17 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32457\tvalid_1's rmse: 1.36494\n",
      "[400]\ttraining's rmse: 1.16588\tvalid_1's rmse: 1.28083\n",
      "[600]\ttraining's rmse: 1.08979\tvalid_1's rmse: 1.26258\n",
      "[800]\ttraining's rmse: 1.03728\tvalid_1's rmse: 1.25757\n",
      "[1000]\ttraining's rmse: 0.993145\tvalid_1's rmse: 1.25636\n",
      "[1200]\ttraining's rmse: 0.952711\tvalid_1's rmse: 1.25695\n",
      "Early stopping, best iteration is:\n",
      "[1022]\ttraining's rmse: 0.988703\tvalid_1's rmse: 1.25621\n",
      "Model training done in 20.68832302093506 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2562123803495224\n",
      "Time passed: 2243.2217853069305 seconds.\n",
      "Fitting big fold 17 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32302\tvalid_1's rmse: 1.42399\n",
      "[400]\ttraining's rmse: 1.17749\tvalid_1's rmse: 1.34816\n",
      "[600]\ttraining's rmse: 1.11245\tvalid_1's rmse: 1.33434\n",
      "[800]\ttraining's rmse: 1.06862\tvalid_1's rmse: 1.33126\n",
      "[1000]\ttraining's rmse: 1.03104\tvalid_1's rmse: 1.33302\n",
      "Early stopping, best iteration is:\n",
      "[860]\ttraining's rmse: 1.0568\tvalid_1's rmse: 1.33095\n",
      "Model training done in 20.56291961669922 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3309456635249621\n",
      "Time passed: 2265.698834180832 seconds.\n",
      "Fitting big fold 17 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32109\tvalid_1's rmse: 1.43439\n",
      "[400]\ttraining's rmse: 1.17691\tvalid_1's rmse: 1.3421\n",
      "[600]\ttraining's rmse: 1.11131\tvalid_1's rmse: 1.32169\n",
      "[800]\ttraining's rmse: 1.06865\tvalid_1's rmse: 1.31639\n",
      "[1000]\ttraining's rmse: 1.03152\tvalid_1's rmse: 1.31503\n",
      "[1200]\ttraining's rmse: 0.996153\tvalid_1's rmse: 1.31488\n",
      "Early stopping, best iteration is:\n",
      "[1125]\ttraining's rmse: 1.00973\tvalid_1's rmse: 1.31389\n",
      "Model training done in 25.01929759979248 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.313887243517548\n",
      "Time passed: 2292.8882908821106 seconds.\n",
      "Fitting big fold 17 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32419\tvalid_1's rmse: 1.40437\n",
      "[400]\ttraining's rmse: 1.18005\tvalid_1's rmse: 1.3085\n",
      "[600]\ttraining's rmse: 1.11354\tvalid_1's rmse: 1.28639\n",
      "[800]\ttraining's rmse: 1.06974\tvalid_1's rmse: 1.27976\n",
      "[1000]\ttraining's rmse: 1.03298\tvalid_1's rmse: 1.27806\n",
      "[1200]\ttraining's rmse: 0.998994\tvalid_1's rmse: 1.27627\n",
      "[1400]\ttraining's rmse: 0.965885\tvalid_1's rmse: 1.2761\n",
      "[1600]\ttraining's rmse: 0.934278\tvalid_1's rmse: 1.2754\n",
      "[1800]\ttraining's rmse: 0.905848\tvalid_1's rmse: 1.27501\n",
      "[2000]\ttraining's rmse: 0.876361\tvalid_1's rmse: 1.27483\n",
      "Early stopping, best iteration is:\n",
      "[1975]\ttraining's rmse: 0.879986\tvalid_1's rmse: 1.27452\n",
      "Model training done in 38.83544039726257 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2745233542019023\n",
      "Time passed: 2334.9155502319336 seconds.\n",
      "Fitting big fold 17 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32927\tvalid_1's rmse: 1.36523\n",
      "[400]\ttraining's rmse: 1.18254\tvalid_1's rmse: 1.27279\n",
      "[600]\ttraining's rmse: 1.11414\tvalid_1's rmse: 1.25015\n",
      "[800]\ttraining's rmse: 1.06869\tvalid_1's rmse: 1.24388\n",
      "[1000]\ttraining's rmse: 1.0296\tvalid_1's rmse: 1.24126\n",
      "[1200]\ttraining's rmse: 0.994259\tvalid_1's rmse: 1.24049\n",
      "Early stopping, best iteration is:\n",
      "[1155]\ttraining's rmse: 1.00133\tvalid_1's rmse: 1.24009\n",
      "Model training done in 25.04540753364563 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2400887242837972\n",
      "Time passed: 2362.1734595298767 seconds.\n",
      "Average big fold 17 error: 1.2717102091305295\n",
      "Total big fold 17 std 0.0343\n",
      "Fitting big fold 18 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31585\tvalid_1's rmse: 1.38797\n",
      "[400]\ttraining's rmse: 1.15596\tvalid_1's rmse: 1.2969\n",
      "[600]\ttraining's rmse: 1.07917\tvalid_1's rmse: 1.27477\n",
      "[800]\ttraining's rmse: 1.02574\tvalid_1's rmse: 1.26817\n",
      "[1000]\ttraining's rmse: 0.981568\tvalid_1's rmse: 1.26763\n",
      "[1200]\ttraining's rmse: 0.939854\tvalid_1's rmse: 1.26751\n",
      "Early stopping, best iteration is:\n",
      "[1060]\ttraining's rmse: 0.968437\tvalid_1's rmse: 1.26726\n",
      "Model training done in 21.41099500656128 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.267255646545922\n",
      "Time passed: 2385.878046274185 seconds.\n",
      "Fitting big fold 18 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33132\tvalid_1's rmse: 1.35369\n",
      "[400]\ttraining's rmse: 1.18948\tvalid_1's rmse: 1.25744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 1.12573\tvalid_1's rmse: 1.23382\n",
      "[800]\ttraining's rmse: 1.08334\tvalid_1's rmse: 1.2249\n",
      "[1000]\ttraining's rmse: 1.04614\tvalid_1's rmse: 1.22405\n",
      "[1200]\ttraining's rmse: 1.01246\tvalid_1's rmse: 1.22133\n",
      "[1400]\ttraining's rmse: 0.981298\tvalid_1's rmse: 1.21891\n",
      "[1600]\ttraining's rmse: 0.951997\tvalid_1's rmse: 1.21701\n",
      "[1800]\ttraining's rmse: 0.923211\tvalid_1's rmse: 1.21604\n",
      "[2000]\ttraining's rmse: 0.896246\tvalid_1's rmse: 1.2146\n",
      "[2200]\ttraining's rmse: 0.870565\tvalid_1's rmse: 1.2146\n",
      "Early stopping, best iteration is:\n",
      "[2006]\ttraining's rmse: 0.895344\tvalid_1's rmse: 1.21439\n",
      "Model training done in 39.97116756439209 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2143903512504235\n",
      "Time passed: 2429.0913548469543 seconds.\n",
      "Fitting big fold 18 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32024\tvalid_1's rmse: 1.42467\n",
      "[400]\ttraining's rmse: 1.17457\tvalid_1's rmse: 1.35038\n",
      "[600]\ttraining's rmse: 1.11035\tvalid_1's rmse: 1.337\n",
      "[800]\ttraining's rmse: 1.06676\tvalid_1's rmse: 1.33485\n",
      "Early stopping, best iteration is:\n",
      "[791]\ttraining's rmse: 1.06843\tvalid_1's rmse: 1.33475\n",
      "Model training done in 19.49928379058838 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3347469348779895\n",
      "Time passed: 2450.501271724701 seconds.\n",
      "Fitting big fold 18 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32509\tvalid_1's rmse: 1.38914\n",
      "[400]\ttraining's rmse: 1.18118\tvalid_1's rmse: 1.29822\n",
      "[600]\ttraining's rmse: 1.11621\tvalid_1's rmse: 1.27482\n",
      "[800]\ttraining's rmse: 1.07213\tvalid_1's rmse: 1.26922\n",
      "[1000]\ttraining's rmse: 1.03459\tvalid_1's rmse: 1.26516\n",
      "[1200]\ttraining's rmse: 1.0008\tvalid_1's rmse: 1.2647\n",
      "[1400]\ttraining's rmse: 0.968008\tvalid_1's rmse: 1.26431\n",
      "Early stopping, best iteration is:\n",
      "[1278]\ttraining's rmse: 0.987651\tvalid_1's rmse: 1.26397\n",
      "Model training done in 27.32792639732361 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.26396598027037\n",
      "Time passed: 2480.2173388004303 seconds.\n",
      "Fitting big fold 18 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32498\tvalid_1's rmse: 1.38273\n",
      "[400]\ttraining's rmse: 1.17988\tvalid_1's rmse: 1.29358\n",
      "[600]\ttraining's rmse: 1.11324\tvalid_1's rmse: 1.27258\n",
      "[800]\ttraining's rmse: 1.06685\tvalid_1's rmse: 1.26687\n",
      "[1000]\ttraining's rmse: 1.02825\tvalid_1's rmse: 1.2655\n",
      "[1200]\ttraining's rmse: 0.991712\tvalid_1's rmse: 1.26488\n",
      "[1400]\ttraining's rmse: 0.956834\tvalid_1's rmse: 1.26323\n",
      "[1600]\ttraining's rmse: 0.925382\tvalid_1's rmse: 1.26355\n",
      "[1800]\ttraining's rmse: 0.895266\tvalid_1's rmse: 1.26339\n",
      "Early stopping, best iteration is:\n",
      "[1671]\ttraining's rmse: 0.914477\tvalid_1's rmse: 1.26281\n",
      "Model training done in 35.4083366394043 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.262806574482776\n",
      "Time passed: 2518.592102766037 seconds.\n",
      "Average big fold 18 error: 1.2693988728018304\n",
      "Total big fold 18 std 0.0384\n",
      "Fitting big fold 19 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31799\tvalid_1's rmse: 1.39084\n",
      "[400]\ttraining's rmse: 1.16757\tvalid_1's rmse: 1.29405\n",
      "[600]\ttraining's rmse: 1.09444\tvalid_1's rmse: 1.27146\n",
      "[800]\ttraining's rmse: 1.04405\tvalid_1's rmse: 1.26629\n",
      "[1000]\ttraining's rmse: 0.998466\tvalid_1's rmse: 1.26461\n",
      "[1200]\ttraining's rmse: 0.958757\tvalid_1's rmse: 1.26346\n",
      "[1400]\ttraining's rmse: 0.921575\tvalid_1's rmse: 1.26372\n",
      "Early stopping, best iteration is:\n",
      "[1228]\ttraining's rmse: 0.953198\tvalid_1's rmse: 1.26331\n",
      "Model training done in 24.52649760246277 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2633143910518643\n",
      "Time passed: 2545.55717420578 seconds.\n",
      "Fitting big fold 19 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3339\tvalid_1's rmse: 1.33923\n",
      "[400]\ttraining's rmse: 1.19324\tvalid_1's rmse: 1.23637\n",
      "[600]\ttraining's rmse: 1.12848\tvalid_1's rmse: 1.21541\n",
      "[800]\ttraining's rmse: 1.08503\tvalid_1's rmse: 1.21114\n",
      "[1000]\ttraining's rmse: 1.04948\tvalid_1's rmse: 1.21071\n",
      "Early stopping, best iteration is:\n",
      "[880]\ttraining's rmse: 1.06951\tvalid_1's rmse: 1.21025\n",
      "Model training done in 19.719738245010376 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2102454772089721\n",
      "Time passed: 2567.116954088211 seconds.\n",
      "Fitting big fold 19 out of 20 and sub fold 3 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.3228\tvalid_1's rmse: 1.41238\n",
      "[400]\ttraining's rmse: 1.17886\tvalid_1's rmse: 1.32759\n",
      "[600]\ttraining's rmse: 1.11357\tvalid_1's rmse: 1.31123\n",
      "[800]\ttraining's rmse: 1.07059\tvalid_1's rmse: 1.30862\n",
      "[1000]\ttraining's rmse: 1.03335\tvalid_1's rmse: 1.30728\n",
      "[1200]\ttraining's rmse: 1.00028\tvalid_1's rmse: 1.3064\n",
      "[1400]\ttraining's rmse: 0.967477\tvalid_1's rmse: 1.30528\n",
      "Early stopping, best iteration is:\n",
      "[1384]\ttraining's rmse: 0.970327\tvalid_1's rmse: 1.30488\n",
      "Model training done in 30.137834787368774 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.3048821888292088\n",
      "Time passed: 2599.7603573799133 seconds.\n",
      "Fitting big fold 19 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32442\tvalid_1's rmse: 1.38654\n",
      "[400]\ttraining's rmse: 1.17734\tvalid_1's rmse: 1.31095\n",
      "[600]\ttraining's rmse: 1.10938\tvalid_1's rmse: 1.29436\n",
      "[800]\ttraining's rmse: 1.06328\tvalid_1's rmse: 1.29108\n",
      "[1000]\ttraining's rmse: 1.02567\tvalid_1's rmse: 1.29046\n",
      "[1200]\ttraining's rmse: 0.989483\tvalid_1's rmse: 1.28877\n",
      "[1400]\ttraining's rmse: 0.955534\tvalid_1's rmse: 1.28894\n",
      "Early stopping, best iteration is:\n",
      "[1318]\ttraining's rmse: 0.969377\tvalid_1's rmse: 1.28822\n",
      "Model training done in 28.26204204559326 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.288218737142931\n",
      "Time passed: 2630.4275641441345 seconds.\n",
      "Fitting big fold 19 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.31974\tvalid_1's rmse: 1.39683\n",
      "[400]\ttraining's rmse: 1.1705\tvalid_1's rmse: 1.31819\n",
      "[600]\ttraining's rmse: 1.1027\tvalid_1's rmse: 1.30156\n",
      "[800]\ttraining's rmse: 1.05644\tvalid_1's rmse: 1.29683\n",
      "[1000]\ttraining's rmse: 1.01872\tvalid_1's rmse: 1.29597\n",
      "Early stopping, best iteration is:\n",
      "[980]\ttraining's rmse: 1.02278\tvalid_1's rmse: 1.29575\n",
      "Model training done in 22.688384771347046 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.295751241935299\n",
      "Time passed: 2655.302638530731 seconds.\n",
      "Average big fold 19 error: 1.2739661705245897\n",
      "Total big fold 19 std 0.03405\n",
      "Fitting big fold 20 out of 20 and sub fold 1 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.30757\tvalid_1's rmse: 1.39723\n",
      "[400]\ttraining's rmse: 1.14818\tvalid_1's rmse: 1.31348\n",
      "[600]\ttraining's rmse: 1.07438\tvalid_1's rmse: 1.29608\n",
      "[800]\ttraining's rmse: 1.02274\tvalid_1's rmse: 1.29184\n",
      "[1000]\ttraining's rmse: 0.980076\tvalid_1's rmse: 1.28983\n",
      "[1200]\ttraining's rmse: 0.939984\tvalid_1's rmse: 1.28971\n",
      "Early stopping, best iteration is:\n",
      "[1063]\ttraining's rmse: 0.967203\tvalid_1's rmse: 1.28959\n",
      "Model training done in 22.194050788879395 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2895939852874043\n",
      "Time passed: 2679.7620158195496 seconds.\n",
      "Fitting big fold 20 out of 20 and sub fold 2 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32645\tvalid_1's rmse: 1.39724\n",
      "[400]\ttraining's rmse: 1.18099\tvalid_1's rmse: 1.31392\n",
      "[600]\ttraining's rmse: 1.11618\tvalid_1's rmse: 1.29687\n",
      "[800]\ttraining's rmse: 1.07241\tvalid_1's rmse: 1.29256\n",
      "[1000]\ttraining's rmse: 1.03711\tvalid_1's rmse: 1.29238\n",
      "[1200]\ttraining's rmse: 1.00403\tvalid_1's rmse: 1.29014\n",
      "[1400]\ttraining's rmse: 0.972368\tvalid_1's rmse: 1.28982\n",
      "[1600]\ttraining's rmse: 0.942051\tvalid_1's rmse: 1.28799\n",
      "[1800]\ttraining's rmse: 0.913364\tvalid_1's rmse: 1.2877\n",
      "Early stopping, best iteration is:\n",
      "[1665]\ttraining's rmse: 0.932246\tvalid_1's rmse: 1.28736\n",
      "Model training done in 33.06642293930054 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2873632953405794\n",
      "Time passed: 2715.6215217113495 seconds.\n",
      "Fitting big fold 20 out of 20 and sub fold 3 out of 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.33013\tvalid_1's rmse: 1.36115\n",
      "[400]\ttraining's rmse: 1.18775\tvalid_1's rmse: 1.26317\n",
      "[600]\ttraining's rmse: 1.123\tvalid_1's rmse: 1.23898\n",
      "[800]\ttraining's rmse: 1.07938\tvalid_1's rmse: 1.23051\n",
      "[1000]\ttraining's rmse: 1.04197\tvalid_1's rmse: 1.22725\n",
      "[1200]\ttraining's rmse: 1.00774\tvalid_1's rmse: 1.22722\n",
      "Early stopping, best iteration is:\n",
      "[1175]\ttraining's rmse: 1.01146\tvalid_1's rmse: 1.22691\n",
      "Model training done in 24.905840158462524 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2269128940237504\n",
      "Time passed: 2742.8653507232666 seconds.\n",
      "Fitting big fold 20 out of 20 and sub fold 4 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32305\tvalid_1's rmse: 1.39748\n",
      "[400]\ttraining's rmse: 1.1775\tvalid_1's rmse: 1.30552\n",
      "[600]\ttraining's rmse: 1.10987\tvalid_1's rmse: 1.28377\n",
      "[800]\ttraining's rmse: 1.06507\tvalid_1's rmse: 1.27978\n",
      "[1000]\ttraining's rmse: 1.02627\tvalid_1's rmse: 1.27958\n",
      "[1200]\ttraining's rmse: 0.99194\tvalid_1's rmse: 1.28032\n",
      "Early stopping, best iteration is:\n",
      "[1018]\ttraining's rmse: 1.02321\tvalid_1's rmse: 1.2792\n",
      "Model training done in 23.454796075820923 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2792014344571911\n",
      "Time passed: 2768.5046451091766 seconds.\n",
      "Fitting big fold 20 out of 20 and sub fold 5 out of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1.32731\tvalid_1's rmse: 1.36886\n",
      "[400]\ttraining's rmse: 1.18222\tvalid_1's rmse: 1.28051\n",
      "[600]\ttraining's rmse: 1.1157\tvalid_1's rmse: 1.26094\n",
      "[800]\ttraining's rmse: 1.07144\tvalid_1's rmse: 1.25704\n",
      "[1000]\ttraining's rmse: 1.03332\tvalid_1's rmse: 1.25701\n",
      "Early stopping, best iteration is:\n",
      "[849]\ttraining's rmse: 1.06167\tvalid_1's rmse: 1.25655\n",
      "Model training done in 20.285988807678223 seconds.\n",
      "17.50439003707821 17.50439003707821\n",
      "Fold error 1.2565458201661\n",
      "Time passed: 2790.718740463257 seconds.\n",
      "Average big fold 20 error: 1.2732991428253335\n",
      "Total big fold 20 std 0.02361\n",
      "Wall time: 46min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_oof_lgb, pred_test_list_lgb, fold_errors, _ = run_calculations(X, test, cv_folds, 'lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error across 20 folds: 1.26893\n",
      "Average std across 20 folds: 0.00604\n",
      "(averaged std of 20 folds: 0.02762)\n",
      "Wall time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avg_error = np.mean(([np.mean(x) for x in fold_errors]))\n",
    "avg_std = np.std(([np.mean (x) for x in fold_errors]))\n",
    "print(f'Average error across 20 folds: {avg_error.round(5)}')\n",
    "print(f'Average std across 20 folds: {avg_std.round(5)}')\n",
    "print(f'(averaged std of 20 folds: {np.mean(([np.std (x) for x in fold_errors])).round(5)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test predictions: 20\n",
      "Length of avg test predictions: 49342\n"
     ]
    }
   ],
   "source": [
    "print('Length of test predictions:', len(pred_test_list_lgb))\n",
    "# avg_pred_test_list_lgb = np.expm1(np.mean(pred_test_list_lgb, axis=0))\n",
    "avg_pred_test_list_lgb = np.mean(pred_test_list_lgb, axis=0)\n",
    "print('Length of avg test predictions:', len(avg_pred_test_list_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107842.8297417948, 19432101.594394095)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pred_test_list_lgb.min(), avg_pred_test_list_lgb.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERRORS\n",
    "# errors = pd.DataFrame(fold_errors)\n",
    "# errors.to_csv(os.path.join(PATH_TO_DAT`A, 'output/tenich_20_fold_errors_1dconvnn_cv1620_std0037.csv'), index=False, header=False)\n",
    "\n",
    "# 20x oof train preds\n",
    "# with open(os.path.join(PATH_TO_DATA, 'output/tenich_20folds_train_1dconvnn_cv1561_std0021.csv'), 'wb') as f:\n",
    "#     pickle.dump(y_oof_lgb, f)\n",
    "    \n",
    "# #20x test preds\n",
    "# with open(os.path.join(PATH_TO_DATA, 'output/tenich_20folds_test_1dconvnn_cv1561_std0021.csv'), 'wb') as f:\n",
    "#     pickle.dump(pred_test_list_lgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 230 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub_df = pd.read_csv(f'{DATA_PATH}sample_submission.csv')#, usecols = ['ID']\n",
    "model_to_submit = pd.DataFrame({ 'ID': sub_df['ID'].values,\n",
    "                            'target': avg_pred_test_list_lgb})\n",
    "model_to_submit.to_csv(f'{PRED_TEST_PATH}{MODEL_NAME}_20folds_test_averaged_cv{avg_error*1000:0>4.0f}_std{avg_std*1000:0>4.0f}.csv',\n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000137c73</td>\n",
       "      <td>6.782998e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00021489f</td>\n",
       "      <td>1.637664e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004d7953</td>\n",
       "      <td>5.344150e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00056a333</td>\n",
       "      <td>7.729168e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00056d8eb</td>\n",
       "      <td>2.359130e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0005fc190</td>\n",
       "      <td>3.197165e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000787e86</td>\n",
       "      <td>3.808208e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0008510a0</td>\n",
       "      <td>4.069301e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000895faf</td>\n",
       "      <td>2.682486e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000986fba</td>\n",
       "      <td>3.279674e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID        target\n",
       "0  000137c73  6.782998e+06\n",
       "1  00021489f  1.637664e+06\n",
       "2  0004d7953  5.344150e+06\n",
       "3  00056a333  7.729168e+06\n",
       "4  00056d8eb  2.359130e+06\n",
       "5  0005fc190  3.197165e+06\n",
       "6  000787e86  3.808208e+06\n",
       "7  0008510a0  4.069301e+06\n",
       "8  000895faf  2.682486e+06\n",
       "9  000986fba  3.279674e+06"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_submit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49332</th>\n",
       "      <td>ffef8aa08</td>\n",
       "      <td>2.492344e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49333</th>\n",
       "      <td>fff0ee67d</td>\n",
       "      <td>3.323852e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49334</th>\n",
       "      <td>fff2aa673</td>\n",
       "      <td>1.615774e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49335</th>\n",
       "      <td>fff479492</td>\n",
       "      <td>1.153307e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49336</th>\n",
       "      <td>fff64bf93</td>\n",
       "      <td>3.187633e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49337</th>\n",
       "      <td>fff73b677</td>\n",
       "      <td>3.230444e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49338</th>\n",
       "      <td>fff7b5923</td>\n",
       "      <td>6.304897e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49339</th>\n",
       "      <td>fff7c698f</td>\n",
       "      <td>4.067467e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49340</th>\n",
       "      <td>fff8dba89</td>\n",
       "      <td>3.212410e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49341</th>\n",
       "      <td>fffbe2f6f</td>\n",
       "      <td>6.405764e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID        target\n",
       "49332  ffef8aa08  2.492344e+06\n",
       "49333  fff0ee67d  3.323852e+06\n",
       "49334  fff2aa673  1.615774e+06\n",
       "49335  fff479492  1.153307e+06\n",
       "49336  fff64bf93  3.187633e+06\n",
       "49337  fff73b677  3.230444e+06\n",
       "49338  fff7b5923  6.304897e+06\n",
       "49339  fff7c698f  4.067467e+06\n",
       "49340  fff8dba89  3.212410e+05\n",
       "49341  fffbe2f6f  6.405764e+06"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_submit.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(PATH_TO_DATA, 'input/test.csv'), usecols=['ID'])\n",
    "\n",
    "lgb = pd.DataFrame({'ID': test['ID'].values,\n",
    "                    'target': avg_pred_test_list_lgb})\n",
    "\n",
    "lgb.to_csv(os.path.join(PATH_TO_DATA, 'output/tenich_ridge_blending_12855_0026.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sub_without_leak = lgb.copy()\n",
    "\n",
    "with open('./05_submission_with_leak_0814/have_data_test.obj', 'rb') as f:\n",
    "    have_data_test = pickle.load(f)\n",
    "    \n",
    "sub_with_leak = pd.read_csv(f'./05_submission_with_leak_0814/kvr777_5f_lgb_from0701_with_leak.csv') \n",
    "\n",
    "best_sub_without_leak.loc[have_data_test, 'target'] = sub_with_leak.loc[have_data_test, 'target'] \n",
    "\n",
    "best_sub_without_leak.to_csv(os.path.join(PATH_TO_DATA, 'output/tenich_ridge_blending_12855_0026_with_leak.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
